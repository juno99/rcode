#고급통계실무
#eda 1-2
pollution <- read.csv(xx, colClasses = c("numeric", "character","factor", "numeric", "numeric"))
summary(pollution$pm25)
boxplot(pollution$pm25, col = "blue")
hist(pollution$pm25, col = "green")
rug(pollution$pm25) #히스토그램 밑부분에 표시
hist(pollution$pm25, col = "green", breaks = 100) #히스토그램 급간 100개로
boxplot(pollution$pm25, col = "blue")
abline(h = 12) #y=12선 추가

hist(pollution$pm25, col = "green")
abline(v = 12, lwd = 2)
abline(v = median(pollution$pm25), col = "magenta", lwd = 4)

barplot(table(pollution$region), col = "wheat", main = "Number of Counties in Each Region")

boxplot(pm25 ~ region, data = pollution, col = "red") #지역별 오염도

par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))  #multiple histogram
hist(subset(pollution, region == "east")$pm25, col = "green")
hist(subset(pollution, region == "west")$pm25, col = "green")

with(pollution, plot(latitude, pm25, col = region)) #지역별로 다른 색깔, scatter plot
abline(h = 12, lwd = 2, lty = 2)

par(mfrow = c(1, 2), mar = c(5, 4, 2, 1)) #mutiple scatterplot
with(subset(pollution, region == "west"), plot(latitude, pm25, main = "West"))
with(subset(pollution, region == "east"), plot(latitude, pm25, main = "East"))


#1-2 
#baseplot
library(datasets)
data(cars)
with(cars, plot(speed, dist))

#lattice
library(lattice)
state <- data.frame(state.x77, region = state.region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))

#ggplot2
library(ggplot2)
data(mpg)
qplot(displ, hwy, data = mpg)


#baseplot
#hist
library(datasets)
hist(airquality$Ozone)
#scatter
library(datasets)
with(airquality, plot(Wind, Ozone))
#boxplot
library(datasets)
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality, xlab = "Month", ylab = "Ozone (ppb)")

library(datasets)
with(airquality, plot(Wind, Ozone))
title(main = "Ozone and Wind in New York City")

with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City")) #5월달 파란색으로 
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))

#plot한뒤 조건 
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City",
                      type = "n"))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = "red"))
legend("topright", pch = 1, col = c("blue", "red"), legend = c("May", "Other Months"))

#plot한뒤 회귀선 그리기 
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City",
                      pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)

#plot한뒤 동시에 그리기 
par(mfrow = c(1, 2))
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
})
#plot한뒤 3개 동시에 그리기 
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
  plot(Temp, Ozone, main = "Ozone and Temperature")
  mtext("Ozone and Weather in New York City", outer = TRUE)
})

par(mfrow=c(1,1))
#1-5
pdf(file = "myplot.pdf") ## Open PDF device; create 'myplot.pdf' in my working directory
## Create plot and send to a file (no plot appears on screen)
with(faithful, plot(eruptions, waiting))
title(main = "Old Faithful Geyser data") ## Annotate plot; still nothing on screen
dev.off() ## Close the PDF file device
## Now you can view the file 'myplot.pdf' on your computer

#pdf로 만들기 
library(datasets)
with(faithful, plot(eruptions, waiting)) ## Create plot on screen device
title(main = "Old Faithful Geyser data") ## Add a main title
dev.copy(png, file = "geyserplot.png")
dev.off()


#2-1 lattice
library(lattice)
library(datasets)
## Simple scatterplot
xyplot(Ozone ~ Wind, data = airquality)

library(datasets)
library(lattice)
## Convert 'Month' to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))

set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c("Group 1", "Group 2"))
xyplot(y ~ x | f, layout = c(2, 1)) ## Plot with 2 panels
panel.abline(h=median(y),lty=2)

#lattice함수 응용 
xyplot(y ~ x | f, panel = function(x, y, ...) {
  panel.xyplot(x, y, ...) ## First call the default panel function for 'xyplot'
  panel.abline(h = median(y), lty = 2) ## Add a horizontal line at the median
})
xyplot(y ~ x | f, panel = function(x, y, ...) {
  panel.xyplot(x, y, ...) ## First call default panel function
  panel.lmline(x, y, col = 2) ## Overlay a simple linear regression line
})


#2-2 ggplot2
library(ggplot2)
qplot(displ,hwy,data=mpg,color=drv)
head(mpg)

qplot(displ,hwy,data=mpg,color=drv)
#add a geom
qplot(displ,hwy,data=mpg,geom=c("point","smooth"))
#hist
qplot(hwy,data=mpg,fill=drv)
#facet
qplot(displ,hwy,data = mpg,facets = .~drv) #가로로 
qplot(hwy,data = mpg,facets =drv~.,binwidth=2)  #세로로 
#density smooth
qplot(x,data,geom="density",color)


qplot(x,y,data,color,geom = c("point","smooth"),method="lm")

#ggplot으로 
g<-ggplot(data,aes(x,y))
g+geom_point(color="steelblue",size,alpha)+geom_smooth(method = "lm")+facet_grid()
g+geom_point(aes(color=bmicat))+labs(title)+labs(x="",y="") #bmicat 데이터 변수

#change the theme
g+geom_point(aes(color=bmicat))+theme_bw(base_family = "Times")

#axis limits
testdat<-data.frame(x=1:100,y=rnorm(100))
testdat[50,2]<-100
plot(testdat$x,testdat$y,type = "l",ylim=c(-3,3))

g<-ggplot(testdat,aes(x=x,y=y))
g+geom_line()

g+geom_line()+ylim(-3,3) #outlier 사라짐 
g+geom_line()+coord_cartesian(ylim = c(-3,3)) #아웃라이어 포함 


#3-1 계층적 군집 
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)

#hclust
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)

myplclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)),
                      hang = 0.1, ...) {
  ## modifiction of plclust for plotting hclust objects *in colour*! Copyright
  ## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector
  ## of labels of the leaves of the tree lab.col: colour for the labels;
  ## NA=default device foreground colour hang: as in hclust & plclust Side
  ## effect: A display of hierarchical cluster with coloured leaf labels.
  y <- rep(hclust$height, 2)
  x <- as.numeric(hclust$merge)
  y <- y[which(x < 0)]
  x <- x[which(x < 0)]
  x <- abs(x)
  y <- y[order(x)]
  x <- x[order(x)]
  plot(hclust, labels = FALSE, hang = hang, ...)
  text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
       col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)
}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))

dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)

#3-2 k-means


set.seed(1234); par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))


dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj)

kmeansObj$cluster #클러스터링된거를 나타내기 

par(mar=rep(0.2,4))  #클러스터별로 색
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)

#heatmap
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj <- kmeans(dataMatrix,centers=3)
par(mfrow=c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)],yaxt="n") #클러스터별로 색 

#####3-3
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

#cluster
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

set.seed(678910)
for (i in 1:40) {
  # flip a coin
  coinFlip <- rbinom(1, size = 1, prob = 0.5)
  # if coin is heads add a common pattern to that row
  if (coinFlip) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
  }
}

par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])


par(mar = rep(0.2, 4))
heatmap(dataMatrix)

#Patterns in rows and columns
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)

#svd
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector",
     pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)


#Variance explained
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained",
     pch = 19)

#Relationship to principal components
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1",
     ylab = "Right Singular Vector 1")
abline(c(0, 1))


#Components of the SVD - variance explained
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1),each=5)}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Prop. of variance explained",pch=19)


#Singular value decomposition - true patterns
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")

#v and patterns of variance in rows
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")


#d and variance explained
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained",
     pch = 19)

#face example
load("data/face.rda")
image(t(faceData)[, nrow(faceData):1])

svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular vector", ylab = "Variance explained")

svd1 <- svd(scale(faceData))
## Note that %*% is matrix multiplication
# Here svd1$d[1] is a constant
approx1 <- svd1$u[, 1] %*% t(svd1$v[, 1]) * svd1$d[1]
# In these examples we need to make the diagonal matrix out of d
approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5])
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[, 1:10])

par(mfrow = c(1, 4))
image(t(approx1)[, nrow(approx1):1], main = "(a)")
image(t(approx5)[, nrow(approx5):1], main = "(b)")
image(t(approx10)[, nrow(approx10):1], main = "(c)")
image(t(faceData)[, nrow(faceData):1], main = "(d)")



#3-4 colorRamp
#[,1] [,2] [,3] corresponds to [Red] [Blue] [Green]
pal <- colorRamp(c("red", "blue"))
pal(0)
pal(1)
pal(0.5)

#colorRampPalette
pal <- colorRampPalette(c("red", "yellow"))
pal(2)
pal(10)

#RColorBrewer and colorRampPalette
library(RColorBrewer)
cols <- brewer.pal(3, "BuGn")
pal <- colorRampPalette(cols)
image(volcano, col = pal(20))

#The smoothScatter function
x<-rnorm(10000)
y<-rnorm(10000)
smoothScatter(x,y)

#Scatterplot with transparency
plot(x,y,col=rgb(0,0,0,0.2),pch=19)



#4-1 case study

load("samsungData.rda")
names(samsungData)[1:12]
table(samsungData$activity)

par(mfrow = c(1, 2), mar = c(5, 4, 1, 1))
samsungData <- transform(samsungData, activity = factor(activity))
sub1 <- subset(samsungData, subject == 1)
plot(sub1[, 1], col = sub1$activity, ylab = names(sub1)[1])
plot(sub1[, 2], col = sub1$activity, ylab = names(sub1)[2])
legend("bottomright", legend = unique(sub1$activity), col = unique(sub1$activity),
       pch = 1)

#Clustering based just on average acceleration
source("myplclust.R")
distanceMatrix <- dist(sub1[, 1:3])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))

#Plotting max acceleration for the first subject
par(mfrow = c(1, 2))
plot(sub1[, 10], pch = 19, col = sub1$activity, ylab = names(sub1)[10])
plot(sub1[, 11], pch = 19, col = sub1$activity, ylab = names(sub1)[11])

source("myplclust.R")
distanceMatrix <- dist(sub1[, 10:12])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))

#Singular Value Decomposition
svd1 = svd(scale(sub1[, -c(562, 563)]))
par(mfrow = c(1, 2))
plot(svd1$u[, 1], col = sub1$activity, pch = 19)
plot(svd1$u[, 2], col = sub1$activity, pch = 19)

#Find maximum contributor
plot(svd1$v[, 2], pch = 19)

#New clustering with maximum contributer
maxContrib <-  which.max(svd1$v[, 2])
distanceMatrix <- dist(sub1[, c(10:12, maxContrib)])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))

#K-means clustering (nstart=1, first try)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6)
table(kClust$cluster, sub1$activity)

kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 1)
table(kClust$cluster, sub1$activity)

kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)

kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)


#Cluster 1 Variable Centers (Laying)
plot(kClust$center[1, 1:10], pch = 19, ylab = "Cluster Center", xlab = "")

plot(kClust$center[4, 1:10], pch = 19, ylab = "Cluster Center", xlab = "")

#regression 1
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)

par(mfrow=c(1,1))
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5) #hist에 평균  그려주기


plot(galton$parent,galton$child,pch=19,col="blue")


y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))

beta1 <- cor(y, x) * sd(x) / sd(y)
beta0 <- mean(x) - beta1 * mean(y)
rbind(c(beta0, beta1), coef(lm(x ~ y)))

#센터링 했을때 기울기 
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])

#정규화했을때 기울기 
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])

data(diamond)
plot(diamond$carat, diamond$price,
     xlab = "Mass (carats)",
     ylab = "Price (SIN $)",
     bg = "lightblue",
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(lm(price ~ carat, data = diamond), lwd = 2)


fit <- lm(price ~ carat, data = diamond)
coef(fit)


#changing scale 1/10
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)

newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx

#잔차
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e -(y - yhat)))

#비선형 
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y); abline(lm(y ~ x))

plot(x, resid(lm(y ~ x)));
abline(h = 0)

#표준편차  구하기(잔차이용)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma

sqrt(sum(resid(fit)^2) / (n - 2)) #잔차 이용

summary(fit)

#정리
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")


coefTable

#confidence interval
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]



#swirl 
library(swirl)
install_from_swirl("Exploratory Data Analysis")
swirl()


#2
head(pollution)
dim(pollution)
summary(pollution$pm25)
1
quantile(ppm)
1
boxplot(ppm,col="blue")


#3
?Devices
with(faithful,plot(eruptions,waiting))
title("Old Faithful Geyser data")
dev.cur()
pdf(file="myplot.pdf") 
with(faithful,plot(eruptions,waiting))
title("Old Faithful Geyser data")
dev.cur()
dev.off()
dev.cur()
with(faithful,plot(eruptions, waiting))
title(main = "Old Faithful Geyser data")
dev.copy(png,"geyserplot.png")
dev.copy(png,file="geyserplot.png")
dev.off()

#4
head(cars)
with(cars,plot(speed,dist))
text(mean(cars$speed),max(cars$dist),"SWIRL rules!")
head(state)
table(state$region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))
xyplot(Life.Exp ~ Income | region, data = state, layout = c(2,2))
head(mpg)
dim(mpg)
table(mpg$model)
qplot(displ,hwy,data=mpg)


#5
head(airquality)
range(airquality$Ozone,na.rm = TRUE)
hist(airquality$Ozone)
table(airquality$Month)
boxplot(Ozone~Month,airquality)
boxplot(Ozone~Month,airquality,xlab="Month",ylab="Ozone (ppb)",col.axis="blue",col.lab="red")
title(main="Ozone and Wind in New York City")
with(airquality,plot(Wind,Ozone))
title(main="Ozone and Wind in New York City")
length(par())
names(par())
par()$pin
par("fg")
par("pch") 
par("lty")
plot(airquality$Wind, type="n",airquality$Ozone) 
title(main="Wind and Ozone in NYC")
may<-subset(airquality,Month==5)
points(may$Wind,may$Ozone,col="blue",pch=17) 
notmay<-subset(airquality,Month!=5)
points(notmay$Wind,notmay$Ozone,col="red",pch=8)
legend("topright",pch=c(17,8),col=c("blue","red"),legend = c("May","Other Months"))
abline(v=median(airquality$Wind),lty=2,lwd=2)
par(mfrow=c(1,2))
plot(airquality$Wind,airquality$Ozone,main="Ozone and Wind")
plot(airquality$Ozone,airquality$Solar.R,main="Ozone and Solar Radiation")
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
plot(airquality$Wind,airquality$Ozone,main="Ozone and Wind")
plot(airquality$Solar.R, airquality$Ozone, main = "Ozone and Solar Radiation")
plot(airquality$Temp,airquality$Ozone ,main="Ozone and Temperature")
mtext("Ozone and Weather in New York City",outer = TRUE)


#6
head(airquality)
xyplot(Ozone~Wind,data=airquality)
xyplot(Ozone ~ Wind, data = airquality, pch=8, col="red", main="Big Apple Data")
xyplot(Ozone~Wind|as.factor(Month),data = airquality,layout=c(5,1))
xyplot(Ozone~Wind|Month,data = airquality,layout=c(5,1))
p<-xyplot(Ozone~Wind,data=airquality)
print(p)
names(p)
mynames[myfull]
p[["formula"]]
p[["x.limits"]]
table(f)
xyplot(y~x|f,layout=c(2,1))
v1
v2
myedit("plot1.R")
myedit("plot2.R")
source(pathtofile("plot2.R"),local=TRUE)
str(diamonds)
table(diamonds$color)
table(diamonds$color,diamonds$cut)
myedit("myLabels.R")
source(pathtofile("myLabels.R"),local=TRUE)
xyplot(price~carat|color*cut,data=diamonds,strip=FALSE,pch=20,xlab=myxlab,ylab=myylab,main=mymain)
xyplot(price~carat|color*cut,data=diamonds,pch=20,xlab=myxlab,ylab=myylab,main=mymain)

#7
sample(colors(),10)
pal<-colorRamp(c("red","blue"))
pal(0)
pal(1)
pal(seq(0,1,len=6))
p1<-colorRampPalette( c("red","blue"))
p1(2)
p1(6)
p2 <- colorRampPalette(c("red","yellow"))
p2(10)
showMe(p1(20))
showMe(p2(20))
showMe(p2(2))
p1
?rgb
p3 <- colorRampPalette(c("blue","green"),alpha=.5)
p3(5)
plot(x,y,pch=19,col=rgb(0,.5,.5))
plot(x,y,pch=19,col=rgb(0,.5,.5,.3))
cols<-brewer.pal(3,"BuGn")
showMe(cols)
pal<-colorRampPalette(cols)
showMe(pal(20))
image(volcano,col=pal(20))
image(volcano,col=p1(20))

#8
str(mpg)
qplot(displ,hwy,data=mpg)
qplot(displ,hwy,data=mpg,color=drv)
qplot(displ,hwy,data=mpg,color=drv,geom=c("point","smooth"))
qplot(y=hwy,data=mpg,color=drv)
myhigh
qplot(drv,hwy,data=mpg,geom="boxplot")
qplot(drv,hwy,data=mpg,geom="boxplot",color=manufacturer)
qplot(hwy, data = mpg, fill = drv)
qplot(displ,hwy,data = mpg,facets = .~drv)
qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)

#9
qplot(displ,hwy,data=mpg,geom=c("point","smooth"),facets = .~drv)
g<-ggplot(mpg,aes(displ,hwy))
summary(g)
g+geom_point() 
g+geom_point() +geom_smooth()
g+geom_point() +geom_smooth(method="lm")
g+geom_point() +geom_smooth(method="lm")+facet_grid(.~drv)
g+geom_point() +geom_smooth(method="lm")+facet_grid(.~drv)+ggtitle("Swirl Rules!")
g+geom_point(color="pink",size=4,alpha=0.5) 
g + geom_point(aes(color = drv), size = 4, alpha = 1/2)
g + geom_point(aes(color = drv)) + labs(title="Swirl Rules!") +labs(x="Displacement", y="Hwy Mileage")
g + geom_point(aes(color = drv), size = 2, alpha = 1/2)+geom_smooth(size=4,linetype=3,method = "lm",se=FALSE)
g + geom_point(aes(color = drv))+theme_bw(base_family = "Times")
plot(myx,myy,type="l",ylim = c(-3,3))
g<-ggplot(testdat,aes(x=myx,y=myy))
g+geom_line()
g + geom_line() + ylim(-3,3)
g + geom_line() + coord_cartesian(ylim = c(-3,3))
g<-ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g+geom_point()
g+geom_point()+facet_grid(drv~cyl,margins = TRUE)
g+geom_point()+facet_grid(drv~cyl,margins = TRUE)+geom_smooth(method="lm",se=FALSE,size=2,color="black")
g+geom_point()+facet_grid(drv~cyl,margins = TRUE)+geom_smooth(method="lm",se=FALSE,size=2,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")


#10
str(diamonds)
qplot(price,data=diamonds)
range(diamonds$price)
qplot(price,data=diamonds,binwidth=18497/30)
brk
counts
qplot(price,data=diamonds,binwidth=18497/30,fill=cut)
qplot(price,data=diamonds,geom = "density")
qplot(price,data=diamonds,geom = "density",color=cut)
qplot(carat,price,data=diamonds)
qplot(carat,price,data=diamonds,shape=cut)
qplot(carat,price,data=diamonds, color=cut)
qplot(carat,price,data=diamonds, color=cut)+geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut)+geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut, facets=.~cut) + geom_smooth(method="lm")
g<-ggplot(diamonds,aes(depth,price))
summary(g)
g+geom_point(alpha=1/3)
cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
diamonds$car2 <- cut(diamonds$carat,cutpoints)
g<-ggplot(diamonds,aes(depth,price))
g+geom_point(alpha=1/3)+facet_grid(cut~ car2)
diamonds[myd,]
g+geom_point(alpha=1/3)+facet_grid(cut~ car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)

#11
dist(dataFrame)
hc<-hclust(distxy)
plot(hc)
plot(as.dendrogram(hc))
abline(h=1.5,col="blue")
abline(h=0.4,col="red")
abline(h=0.05,col="green")
dist(dFsm)
hc
heatmap(dataMatrix,col=cm.colors(25))
heatmap(mt)
mt
plot(denmt)
distmt

#12
cmat
points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
mdist(x,y,cx,cy)
apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
tapply(x,newClust,mean)
tapply(y,newClust,mean)
points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)




#13
head(dataMatrix)
heatmap(dataMatrix)
source("addPatt.R",local = TRUE)
heatmap(dataMatrix)
mat
svd(mat)
matu%*%diag%*%t(matv)
svd(scale(mat))
prcomp(scale(mat))
svd1$v[,1]
svd1$d
head(constantMatrix)
svd2$d
svd2$v[,1:2]
svd2$d
dim(faceData)
a1<-(svd1$u[,1] * svd1$d[1]) %*% t(svd1$v[,1])
myImage(a1)
a2 <- svd1$u[,1:2] %*% diag(svd1$d[1:2]) %*% t(svd1$v[,1:2])
myImage(a2)
myImage(svd1$u[,1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[,1:5]))
myImage(svd1$u[,1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[,1:10]))

#14
dim(ssd)
names(ssd[,562:563])
table(ssd$subject)
sum(table(ssd$subject))
table(ssd$activity)
sub1 <- subset(ssd, subject == 1) 
dim(sub1)
names(sub1[,1:12])
myedit("showXY.R")
showMe(1:6)
mdist <- dist(sub1[,1:3])
hclustering<-hclust(mdist)
myplclust(hclustering, lab.col = unclass(sub1$activity))
mdist <- dist(sub1[,10:12]) 
hclustering<-hclust(mdist)
myplclust(hclustering, lab.col = unclass(sub1$activity))
svd1 <- svd(scale(sub1[,-c(562,563)]))
dim(svd1$u)
maxCon <- which.max(svd1$v[,2])
mdist <- dist(sub1[,c(10:12,maxCon)])
hclustering<-hclust(mdist)
myplclust(hclustering, lab.col = unclass(sub1$activity))
names(sub1)[maxCon]
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6)
table(kClust$cluster,sub1$activity)
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart=100)
table(kClust$cluster,sub1$activity)
dim(kClust$centers)
laying <- which(kClust$size==29)
plot(kClust$centers[laying, 1:12],pch=19,ylab="Laying Cluster")
names(sub1[,1:3])
walkdown <- which(kClust$size==49)
plot(kClust$centers[walkdown, 1:12],pch=19,ylab="Walkdown Cluster")


#15
dim(pm0)
head(pm0)
print(cnames)
cnames<-strsplit(cnames,"|",fixed = TRUE)
cnames
names(pm0)<-make.names(cnames[[1]][wcol])
head(pm0)
x0<-pm0$Sample.Value
str(x0)
mean(is.na(x0))
names(pm1)<-make.names(cnames[[1]][wcol])
dim(pm1)
x1<-pm1$Sample.Value
mean(is.na(x1))
summary(x0)
summary(x1)
boxplot(x0,x1)
boxplot(log10(x0),log10(x1))
negative<-x1<0
sum(negative,na.rm = TRUE)
mean(negative,na.rm = TRUE)
dates<-pm1$Date
str(dates)
dates<-as.Date(as.character(dates),"%Y%m%d")
head(dates)
hist(dates[negative],"month")
str(site0)
both<-intersect(site0,site1)
both
head(pm0)
cnt0<-subset
cnt0 <- subset(pm0, State.Code == 36 & county.site %in% both)
cnt1 <- subset(pm1, State.Code == 36 & county.site %in% both)
sapply(split(cnt0, cnt0$county.site), nrow)
sapply(split(cnt1, cnt1$county.site), nrow)
pm0sub<-subset(cnt0,County.Code ==63&Site.ID==2008)
pm1sub<-subset(cnt1,County.Code ==63&Site.ID==2008)
x0sub<-pm0sub$Sample.Value
x1sub<-pm1sub$Sample.Value
dates0 <- as.Date(as.character(pm0sub$Date),"%Y%m%d")
dates1 <- as.Date(as.character(pm1sub$Date),"%Y%m%d")
par(mfrow=c(1,2),mar=c(4,4,2,1))
plot(dates0, x0sub, pch = 20)
abline(h=median(x0sub,na.rm = TRUE),lwd=2)
plot(dates1, x1sub, pch = 20)
abline(h=median(x1sub,na.rm = TRUE),lwd=2)
rng<-range(x0sub,x1sub,na.rm = TRUE)
rng
mn0<-with(pm0,tapply(Sample.Value,State.Code,mean,na.rm=TRUE))
str(mn0)
mn1<-with(pm1,tapply(Sample.Value,State.Code,mean,na.rm=TRUE))
str(mn1)
summary(mn0)
summary(mn1)
d0<-data.frame(state=names(mn0),mean=mn0)
d1<-data.frame(state=names(mn1),mean=mn1)
mrg<-merge(d0,d1,by="state")
dim(mrg)
head(mrg)
with(mrg,plot(rep(1,52),mrg[,2],xlim = c(.5,2.5)))
with(mrg,points(rep(2,52),mrg[,3]))
segments(rep(1, 52), mrg[, 2], rep(2, 52), mrg[, 3])
mrg[mrg$mean.x < mrg$mean.y, ]



#회귀모형

#1
install_from_swirl("Regression Models")
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)


#2
fit<-lm(child~parent,galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope<-fit$coef[2]

lhs-rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes<-var(fit$residuals)
varEst<-var(est(ols.slope,ols.ic))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)

#3
cor(gpa_nor,gch_nor)
l_nor<-lm(gch_nor~gpa_nor)






#퀴즈

##2
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)

fit<-lm(y~x-1)
summary(fit)

##3
data(mtcars)
fit1<-lm(mpg~wt,mtcars)
summary(fit1)

##6
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(8.58-mean(x))/sd(x)



x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)

fit2<-lm(y~x)
summary(fit2)

x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)


#4
fit<-lm(child ~ parent,galton)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))

mu<-mean(galton$child)
sTot<-sum((galton$child-mu)^2)
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
cor(galton$child,galton$parent)^2

#5
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent -1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
View(trees)
fit<-lm(Volume ~ Girth + Height + Constant -1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lapply(list(fit, fit2), coef)



#6
all <- lm(Fertility ~ ., swiss)
summary(all)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Examination,swiss$Education)
cor(swiss$Agriculture,swiss$Education)
makelms()
ec<-swiss$Examination+swiss$Catholic
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients-efit$coefficients


x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)

fit<-lm(y~x-1)
summary(fit)

data(mtcars)
head(mtcars)
fit<-lm(mpg~wt,mtcars)
summary(fit)


x <- c(8.58, 10.46, 9.01, 9.64, 8.86)

(8.58-mean(x))/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
b1 <- cor(x,y)*sd(y)/sd(x)
b0 <- mean(y) - b1 * mean(x)
fit<-lm(y~x)
summary(fit)
b0

x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)

x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)

fit<-lm(y~x)
fit<-lm(mpg~wt,mtcars)
summary(fit)
predict(fit,data.frame(wt=mean(mtcars$wt)),interval = "confidence")

help(mtcars)
predict(fit,data.frame(wt=3),interval = "predict")
fit1 <- lm(mpg ~ I(wt/2),mtcars)
confint(fit1)
summary(fit1)


fit1<-lm(mpg~1,mtcars)
fit2<-lm(mpg~wt,mtcars)
sum(resid(fit2)^2)/sum(resid(fit1)^2)

#기말고사 준비 
#7
6
B
dim(InsectSprays)
head(InsectSprays,15)
A
sA
summary(InsectSprays[,2])
sapply(InsectSprays,class)
fit<-lm(count~spray,InsectSprays)
summary(fit)$coef
est<-summary(fit)$coef[,1]
mean(sA)
4
mean(sB)
nfit<-lm(count~spray-1,InsectSprays)
summary(nfit)$coef
2
2
spray2 <- relevel(InsectSprays$spray,"C")
fit2<-lm(count~spray2,InsectSprays)
summary(fit2)$coef
1
mean(sC)
3
(fit$coef[2]-fit$coef[3])/1.6011

#8
dim(hunger)
948
names(hunger)
fit<-lm(hunger$Numeric ~ hunger$Year)
summary(fit)$coef
3
1
1
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
2
lmBoth<-lm(hunger$Numeric ~ hunger$Year+hunger$Sex)
lmBoth<-lm(hunger$Numeric ~ hunger$Year+hunger$Sex)
summary(lmBoth)
1
1
1
lmInter<-lm(hunger$Numeric ~ hunger$Year+hunger$Sex+hunger$Year*hunger$Sex)
summary(lmInter)
2
3
1
4
#9
fit<-lm(y~x,out2)
plot(fit, which=1)
2
3
fitno<-lm(y~x,out2[-1,])
plot(fitno, which=1)
coef(fit)-coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
1
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
a<- resid(fit)[1]/(sigma1 * sqrt(1-hatvalues(fit))[1])
head(rstudent(fit))

dy<-predict(fitno, out2)-predict(fit, out2)
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)

#10
2
1
3
2
rgp1()
1
rgp2()
head(swiss)
mdl<-lm(Fertility~.,swiss)
vif(mdl)
mdl2<-lm(Fertility~.-Examination,swiss)
vif(mdl2)
3
3
3

#11
2
1
x1c<-simbias()
apply(x1c, 1, mean)
fit1<-lm(Fertility~Agriculture,swiss)
fit3<-lm(Fertility~Agriculture+Examination+Education,swiss)
anova(fit1,fit3)
2
3
deviance(fit3)
d<-deviance(fit3)/43
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
2
3
2

#12
3
4
View(ravenData)
mdl<-glm(ravenWinNum ~ ravenScore,family = 'binomial',ravenData)
lodds<-predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
exp(confint(mdl))
2
1
anova(mdl)
qchisq(0.95, 1)

#13
var(rpois(1000, 50))
1
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda<-mdl$fitted.values[704]
qpois(.95, lambda)
mdl2
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset= log(visits + 1))
qpois(.95,mdl2$fitted.values[704])
3
1
2



#고통실 기말고사

#ML1

#ML2

#2-1 caret
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE) #75%train, 25% test
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)   #Fit a model
modelFit <- train(type ~.,data=training, method="glm")
modelFit
modelFit <- train(type ~.,data=training, method="glm")
modelFit$finalModel
predictions <- predict(modelFit,newdata=testing)  #prediction
predictions
confusionMatrix(predictions,testing$type) #confusion matrix


#2-2
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE) 
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)

#k-fold
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                     list=TRUE,returnTrain=TRUE)   
sapply(folds,length)
folds[[1]][1:10]   #k-fold에 있는 값 
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                     list=TRUE,returnTrain=FALSE)  #return flase면 test fold
sapply(folds,length)
folds[[1]][1:10]
#resampling (train 중복)
set.seed(32323)
folds <- createResample(y=spam$type,times=10,
                        list=TRUE)
sapply(folds,length)
folds[[1]][1:10]

set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,
                          horizon=10)
names(folds)
folds$train[[1]]
folds$test[[1]]

#2-4 plot train data
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)


inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)

featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")

qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)

qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)

install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage,g=3)  #3구간으로 나눈다(quantile)
table(cutWage)

p1 <- qplot(cutWage,age, data=training,fill=cutWage,
            geom=c("boxplot"))
p1

p2 <- qplot(cutWage,age, data=training,fill=cutWage,
            geom=c("boxplot","jitter"))
p2
install.packages("gridExtra")
library(gridExtra)
grid.arrange(p1,p2,ncol=2)  #p1과 p2를 동시에 그리기 

t1 <- table(cutWage,training$jobclass)
t1
prop.table(t1,1)  #1이면 행의 비유

qplot(wage,colour=education,data=training,geom="density")

#2-5 
#데이터 전처리 
set.seed(32323)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")

mean(training$capitalAve)
sd(training$capitalAve)
#정규화 
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS)
sd(trainCapAveS)

testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(testCapAveS)
sd(testCapAveS)

preObj <- preProcess(training[,-58],method=c("center","scale"))  #정규화랑 똑같다 
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)

testCapAveS <- predict(preObj,testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)

set.seed(32343)   #train에서 정규화 
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
modelFit

preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)

set.seed(13343)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

install.packages("RANN")
library(RANN)
# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve

# Standardize true values

capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA]) 

#2-6
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2

library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]

table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training) #더미변수 코딩 
head(predict(dummies,newdata=training))

nsv <- nearZeroVar(training,saveMetrics=TRUE)  #remove zero
nsv

library(splines)
bsBasis <- bs(training$age,df=3)  #age를 3차원으로 분해 
bsBasis

lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)

predict(bsBasis,age=testing$age)

#2-7

library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)

names(spam)[c(34,32)]
plot(spam[,34],spam[,32])

X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)

smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])

rComp$rotation

typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")

preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)

preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)

testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))

modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))

#2-8
library(caret);data(faithful);
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
                               p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)

plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")


lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)

plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted,lwd=3)  #회귀선 그리기 

#predict a new value
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting=80)
predict(lm1,newdata)

par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)

# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))



#predicton intervals
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty = c(1,1,1), lwd=3)
#same process with carat
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)

#2-9

library(ISLR); library(ggplot2); library(caret);
data(Wage)
Wage <- subset(Wage,select=-c(logwage))
summary(Wage)


inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)


featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")

qplot(age,wage,data=training)

qplot(age,wage,colour=jobclass,data=training)

qplot(age,wage,colour=education,data=training)

modFit<- train(wage ~ age + jobclass + education,
               method = "lm",data=training)
finMod <- modFit$finalModel
print(modFit)


#diagnoistic
plot(finMod,1,pch=19,cex=0.5,col="#00000010")


qplot(finMod$fitted,finMod$residuals,colour=race,data=training)

plot(finMod$residuals,pch=19)

pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)


modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)

#quiz2

install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)


library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]



library(Hmisc)
cols <- colnames(training)
subCols <- cols[-length(cols)] #all but CompressiveStrength
plotCols = 2
par(mfrow = c(ceil(length(subCols)/plotCols), plotCols))
res <- sapply(subCols, function(colName){
  cut <- cut2(training[,colName])
  lab <- paste0("index: col=",colName)
  plot(training$CompressiveStrength, pch=19, col=cut, xlab=lab, ylab="CompressiveStrength")
})


library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]

training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]

histogram(log(training$Superplasticizer))

library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

#용재답 
training2 <- training[,grep("IL",names(training))]
training2 <- training2[,-13]
preProc <- preProcess(training2,method=c("pca","scale"),thresh=0.9)
preProc$numComp

prComp <- prcomp(training2,scale=T)
summary(prComp)
#인터넷 
IL_Colnames = grep("^IL", colnames(training), value=TRUE,ignore.case=TRUE)
pcaMod <- preProcess(training[,IL_Colnames], method="pca", thresh=0.9)
pcaMod

#2-5
head(training)

trainNONPC <- subset(training, select = c(diagnosis, IL_11:IL_8))
testNONPC <- subset(testing, select = c(diagnosis, IL_11:IL_8))

preProc <- preProcess(trainNONPC[,-1], method = "pca", thresh = 0.8) #80%variance 설명 
trainPC <- predict(preProc, trainNONPC[,-1])
testPC <- predict(preProc, testNONPC[,-1])

model1 <- train(x = trainNONPC[-1], y= trainNONPC$diagnosis, method = "glm") #주성분 x
model2 <- train(x = trainPC, y = trainNONPC$diagnosis, method = "glm") #주성분 

confusionMatrix(testNONPC$diagnosis, predict(model1, testNONPC[-1]))
confusionMatrix(testNONPC$diagnosis, predict(model2, testPC))


#ML3

#3-1
install.packages("caret")
library(caret)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)

inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

qplot(Petal.Width,Sepal.Width,colour=Species,data=training)

library(caret)
library(e1071)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)


plot(modFit$finalModel, uniform=TRUE, 
     main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)

install.packages("rattle")
install.packages("RGtk2")
install.packages("rpart")
install.packages("rpart.plot")
library(rattle)
library(rpart)
library(rpart.plot)
library(RGtk2)
library(e1071)
fancyRpartPlot(modFit$finalModel) #버전 업그레이드

remove.packages("RGtk2") 

remove.packages("rattle") 
 
library(rattle) 
predict(modFit,newdata=testing)

#3-2
install.packages("ElemStatLearn")
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)

ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
  ss <- sample(1:dim(ozone)[1],replace=T)
  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
  loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
  ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}

plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
install.packages("party")
library(party)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                       predict = ctreeBag$pred,
                                       aggregate = ctreeBag$aggregate))


plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")

ctreeBag$fit

ctreeBag$pred

ctreeBag$aggregate

#3-3
#random forest
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit

getTree(modFit$finalModel,k=2)


irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)

pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
               

qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")

#3-4

library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]


modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)

qplot(predict(modFit,testing),wage,data=testing)

#3-5
data(iris); library(ggplot2)
names(iris)
table(iris$Species)


inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ ., data=training,method="nb")
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)

equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)


#quiz3
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)

inTrain <- createDataPartition(y=segmentationOriginal$Case,
                               p=0.6, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)

library(rpart)
modFit <-train(Class~.,method="rpart",data = training)
modFit$finalModel
library(rattle)       
library(rpart.plot)

fancyRpartPlot(modFit$finalModel)

install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]



library(caret)

library(rpart)
treeModel <- train(Area ~ ., data=olive, method="rpart")
print(treeModel$finalModel)
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata)

#3-4
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train <- sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA <- SAheart[train,]
testSA <- SAheart[-train,]

set.seed(13234)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
logitModel
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
# Test Set Misclassification rate
missClass(testSA$chd, predictTest)
# Training Set Misclassification rate
missClass(trainSA$chd, predictTrain)

#3-5
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)

install.packages("randomForest")
library(randomForest)
head(vowel.train)
head(vowel.test)
dim(vowel.train)
dim(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)

#ML4
#4 - 1
library(ElemStatLearn); data(prostate)
str(prostate)

small = prostate[1:5,]
lm(lpsa ~ .,data =small)

#4- 2
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))

# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

inTrain <- createDataPartition(y=buildData$wage,
                               p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]

dim(training)
dim(testing)
dim(validation)

mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
              data=training, 
              trControl = trainControl(method="cv"),number=3)

pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)

predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)

sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))


pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)

sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))

#4- 3
install.packages("quantmod")
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)

#4-4

data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                               p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)

table(kMeans1$cluster,training$Species)

modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)

#quiz
#1
library(ElemStatLearn)
library(caret)
library(e1071)

data(vowel.train)

data(vowel.test)

vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
t_rf <- train(y ~ ., data = vowel.train, method = "rf")
t_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
pred_rf <- predict(t_rf, vowel.test)
pred_gbm <- predict(t_gbm, vowel.test)

confusionMatrix(pred_rf, vowel.test$y)$overall[1]
confusionMatrix(pred_gbm, vowel.test$y)$overall[1]

pred2 <- data.frame(pred_rf, pred_gbm, y = vowel.test$y)
sum(pred_rf[pred2$pred_rf == pred2$pred_gbm] == 
      pred2$y[pred2$pred_rf == pred2$pred_gbm]) / sum(pred2$pred_rf == pred2$pred_gbm)

library(gbm)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]


#2
set.seed(62433)
t_rf <- train(diagnosis ~ ., data = training, method = "rf")
t_gbm <- train(diagnosis ~ ., data = training, method = "gbm")
t_lda <- train(diagnosis ~ ., data = training, method = "lda")
pred_rf <- predict(t_rf, testing)
pred_gbm <- predict(t_gbm, testing)
pred_lda <- predict(t_lda, testing)
pred2 <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
a <- train(diagnosis ~ ., method = "rf", data = pred2)
a_Pred <- predict(a, pred2)

confusionMatrix(pred_rf, testing$diagnosis)$overall[1]
confusionMatrix(pred_gbm, testing$diagnosis)$overall[1]
confusionMatrix(pred_lda, testing$diagnosis)$overall[1]
confusionMatrix(a_Pred, testing$diagnosis)$overall[1]


#3
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[inTrain, ]
testing = concrete[-inTrain, ]

set.seed(233)
t_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
library(elasticnet)
plot.enet(t_lasso$finalModel, xvar = "penalty", use.color = T)


#회귀 포아송

par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = "h", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = "h", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = "h", frame = FALSE) 

x <- 0 : 10000; lambda = 3
mu <- sum(x * dpois(x, lambda = lambda))
sigmasq <- sum((x - mu)^2 * dpois(x, lambda = lambda))
c(mu, sigmasq)


#회귀3 quiz
head(mtcars)
str(mtcars)

fit<-lm(mpg~wt+factor(cyl),mtcars)
summary(fit)
fit2<-lm(mpg~factor(cyl),mtcars)
summary(fit2)

fit3<-lm(mpg~wt*factor(cyl),mtcars)
anova(fit,fit3)


lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)


#5
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)

fit <- lm(y~x)
influence(fit)$hat

#6
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)

fit <- lm(y~x)
influence.measures(fit)





#quiz4

#1
library(MASS)
shuttle
str(shuttle)
shuttle$use.binary <- as.integer(shuttle$use == "auto")
fit <- glm(use.binary ~ wind - 1, data = shuttle, family = binomial)
summary(fit)$coef
exp(coef(fit))[1]/exp(coef(fit))[2]

#2
fit <- glm(use.binary ~ wind+magn - 1, data = shuttle, family = binomial)
summary(fit)
summary(fit)$coef

exp(3.635093e-01)/exp(3.955180e-01)

#3
fit1<- glm(1-use.binary~wind-1,data=shuttle,family=binomial)
summary(fit1)

#4
fit<- glm(count~factor(spray)-1,family="poisson",data=InsectSprays)
summary(fit)

#5

set.seed(1234)
t<- rnorm(72)
t1<- log(10)+t
fit<- glm(count~factor(spray)+offset(t),family="poisson",data=InsectSprays)
fit1<- glm(count~factor(spray)+offset(t1),family="poisson",data=InsectSprays)
summary(fit)$coef[,1]
summary(fit1)$coef[,1]
#6
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots<-c(0)
splineTerms<-sapply(knots,function(knot) (x>knot)*(x-knot))
xmat<-cbind(1,x,splineTerms)
fit<-lm(y~xmat-1)
yhat<-predict(fit)
summary(fit)$coef
(yhat[10]-yhat[6])/4


#로지스틱, 포아송
#3-2
rm(list=ls())
load("ravensData.rda")
head(ravensData)

lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef

logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)

plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")

exp(logRegRavens$coeff)
exp(confint(logRegRavens))
anova(logRegRavens,test="Chisq")


#3-3
rm(list=ls())
load("gaData.rda")
gaData$julian <- julian(gaData$date)
head(gaData)

plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")


## linear regression
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col="red",lwd=3)

lm2 <- lm(I(log(gaData$visits + 1)) ~ gaData$julian)

round(exp(coef(lm2)), 5)


## poisson regression
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")
abline(lm1,col="red",lwd=3); lines(gaData$julian,glm1$fitted,col="blue",lwd=3)


## mean-variance relationship
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")


library(sandwich)
confint.agnostic <- function (object, parm, level = 0.95, ...)
{
  cf <- coef(object); pnames <- names(cf)
  if (missing(parm))
    parm <- pnames
  else if (is.numeric(parm))
    parm <- pnames[parm]
  a <- (1 - level)/2; a <- c(a, 1 - a)
  pct <- stats:::format.perc(a, 3)
  fac <- qnorm(a)
  ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                             pct))
  ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
  ci[] <- cf[parm] + ses %o% fac
  ci
}

confint(glm1)
confint.agnostic(glm1)


### offset
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)

plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date",
     ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)


.libPaths()  #패키치 설치 경로 

if(!require(jsonlite)) install.packages("jsonlite"); library(jsonlite)
if(!require(httr)) install.packages("httr"); library(httr)