rm(list=ls())#all delet
exam3 <-read.table(file="clipboard", sep="\t", header=T)#클립보
cs<-read.table("dataCustomers.tab", sep="\t", header=T, stringsAsFactors=F)
demo <-read.csv("userDemoInfo.csv", stringsAsFactors=FALSE) #csv
tail(subset(cs,gender=="여"&marriage=="기혼"&age>=50&age<=59),6)#행뽑기
aggregate(cs[3],by=list(gender=cs$gender),mean) #자료요약
am<-aggregate(tr[8],by=list(ID=tr$custid),sum)#총구매액 합계
order<-order(am$amount,decreasing = T)#구매액 내림차순
head(am[order,],10)#구매액 내림차순순서

cs[!duplicated(cs$residence),]$residence #중복값 없애고 벡터 형태로
unique(cs$residence) #10번 같음
tr$import_name<-ifelse(tr$import==1, "수입품","국산품") #수입품 국산품 필드 만들기
table(tr$store, tr$import_name) #자료요약
tr<-transform(tr,pro=ifelse(import==1,"수입품","국산품")) #transform사용해서 필드추가
mg <-merge(tr, cs, by="custid") #데이터프레임 합체
a <-data.frame(GENDER="남", MIN=min(x$amount), MEDIAN=median(x$amount), MAX=max(x$amount)) #데이터프레임 만들기
mg1$ym <-paste(substr(mg.1$datetime,1,4), substr(mg.1$datetime,6,7), sep="") #날짜 합치기
cast(mg1,custid~ym,length,value="amount")#고객들의 월별 구매건수 계산
tr$wd<-weekdays(as.Date(substr(tr$datetime,1,10),"%Y-%m-%d"),abbreviate = T) #날짜를 요일ㄹ
tr$wd<-factor(tr$wd, levels=c("월","화","수","목","금","토","일")) #순서대로 요인이 나오게
cast(tr, store ~ wd, sum, value="amount") #지점별 요일별 판매액


#탐색 
barplot(ho$Dogs.eaten,names.arg = ho$Year,xlab="Year",ylab="hot dog and buns eaten",main="Nathan`s Hot Dog Eating Contest Result,1980-2010",las=2)
colors<-rep("blue",dim(ho)[1])  #blue인 백터 설정 
colors[ho$Country!="United States"]="red" #미국이 아니면 빨간색 
legend("topleft",legend=c("USA","Others"),fill=c("red","blue")) #레전드 별로 색깔 설정  

ho<-read.csv("http://datasets.flowingdata.com/hot-dog-places.csv")
b<-as.matrix(ho)
ni<-colnames(b)
ni<-substr(ni,4,5)
ni<-paste("`",ni,sep="")
barplot(b,names.arg = ni,xlab="Year",ylab="Hot dogs and burns eaten",main="Hot Dog Eating Contest",col=rainbow(3),legend.text = c(1,2,3),args.legend = list(x = "topleft"))
#선그래프
po<-read.csv("http://datasets.flowingdata.com/world-population.csv")
plot(po$Year,po$Population/1e+09,type="l",xlab="Year",ylab="Population",ylim=c(0,7))
points(po$Year,po$Population/1e+09,pch=20)
mtext("(단위:인구 10억)",side=2,line=3,at=0,cex=0.8) #plot 가장자리에 문자 쓰기
#계단식
po<-read.csv("http://datasets.flowingdata.com/us-postage.csv")
plot(po$Year,po$Price,type="s")
points(po$Year,po$Price,pch=21)
#시계열
unts<-ts(un$Value,start = c(1948,1),frequency = 12) 
plot(unts,type="l",ylim=c(0,11),ylab="Unemployment Rate",xlab="Year")
lines(lowess(unts,f=1/7),col=3,lwd=1.3) #추세선 그리기 
#계절형 시계열 분해
ex<-read.table("Export_1988.txt",header=T)
exts<-ts(ex$Series/1000,start = c(1988,1),frequency = 12)
le<-log(exts) #크기 줄일려고 로그 변환
dle<-decompose(le) #추세,계절성,잡음의 분해 
plot(dle$trend-dle$seasonal) #계절성 빼고 그리기 

#자기상관,교차상관
library(MASS)
acf(geyser$waiting)
ccf(geyser$waiting,geyser$duration)
#파이차트
sum<-sum(da$Number)
da$Number<-da$Number/sum
name<-paste(name,",",round(da$Number*100,1),"%",sep="")
par(mar=c(5,4,4,4)) #여백(그래프 위치 조정)
pie(da$Number,labels=name,col=brewer.pal(10,"RdGy"),main="Area of Interest")
#2원빈도표
a<-UCBAdmissions
tab1<-a[1,,]
tab2<-a[2,,]
tab<-tab1+tab2
par(mar=c(6.5,4,4,6),xpd=T) #범례 밖으로 빼내기 
tab<-prop.table(tab,2) #열방향으로 비율계산(1일 경우 행방향) 
barplot(tab,col=grey(c(0.3,0.9)))
legend(7.5,1.1,legend=rownames(tab),fill=grey(c(0.3,0.9)),yjust=1)
tabt<-t(tab)
barplot(tabt,beside = T,col=brewer.pal(6,"Set3"))
legend(14,0.35,legend=rownames(tabt),fill=brewer.pal(6,"Set3")) #범례 밖으로 빼낼려고 좌표 사용,문자도 사용

tabm<-a[,1,]
tabf<-a[,2,]
tabmp<-prop.table(tabm,2)
tabfp<-prop.table(tabf,2)

barplot(tabmp,main="man",col=1,density = c(0,30),angle = c(45,135))
barplot(tabfp,main="Woman",col=1,density = c(0,30),angle=c(45,135))
legend(7.5,0.8,col=1,legend=rownames(tabm),density=c(0,30),angle = c(45,135))
#모자이크
mosaicplot(~Dept+Gender,data=a,col=T)
tabmp<-as.table(tabmp) #테이블로 만들어주기
tabfp<-as.table(tabfp)
mosaicplot(~Dept+Admit,data=tabmp,color=T,main="Male")
mosaicplot(~Dept+Admit,data=tabfp,color=T,main="Female")
#트리맵 
map.market(id=po$id,area = po$views,group = po$category,color = po$comments,main="FlowingData Map")
#누적연속 그래프
pop=read.csv("us-population-by-age.csv")
library(ggplot2)
pop2<-reshape(pop,direction = "long",varying = list(2:6),times=c("Under 5","5 to 19","20 to 44","45 to 64","Over 65")) #자료 처리
pop2$time<-factor(pop2$time,levels=c("Over 65","45 to 64","20 to 44","5 to 19","Under 5"))
ggplot(pop2,aes(x=Year,y=Under.5,group=time,fill=time))+geom_area(position="fill")+scale_fill_brewer(palette = "Blues",name="Ages")
ggplot(pop2,aes(x=Year,y=Under.5,group=time,color=time))+geom_line(size=1.5)+geom_point()+scale_colour_brewer(palette = "RdYlBu",name="Ages") #선그래프 

#연습문제1 
di<-read.csv("marry.csv")
di<-di[-17,]
rownames(di)<-di[,1]
name<-colnames(di)
name<-substr(name,2,7)
name<-gsub("\\.","-",name)
dit<-t(di)
barplot(dit,names.arg=rownames(di),legend.text = name,col=rainbow(5),las=2)

im<-read.csv("import.csv")
imts<-ts(im$import,start = c(1988,1),frequency = 12)
plot(imts)
lines(lowess(imts,f=1/8),col=2)
lim<-log(imts)
dlim<-decompose(lim)
plot(dlim$x-dlim$seasonal)
lines(lowess(dlim$x-dlim$seasonal,f=1/5),col=3)
plot(dlim$random)
#2
sa<-read.csv("sales.csv")
a<-table(sa$CT1)
sum<-sum(a)
aa<-a/sum
name<-names(a)
name<-paste(name,",",round(aa*100,1),"%",sep="")
pie(aa,labels = name)
tab=xtabs(~CT1,data=sa)

a<-table(sa$Location1,sa$CT1)
barplot(a,beside=T,legend.text = c("경기","부산","서울"),col=brewer.pal(3,"Set3"),args.legend=list(x="topleft"))
tab2=xtabs(~CT1+Location1,sa)
library(RColorBrewer)
barplot(t(tab2),beside=T,col=brewer.pal(3,"Set3"),legend.text = c("경기","부산","서울"),args.legend=list(x="topleft"))

library(plyr)
sm<-ddply(sa,~month+Location1,summarise,price=sum(Price))
library(ggplot2)
ggplot(sm,aes(x=factor(month),y=price,group=CT1,color=CT1))+geom_line(size=1.5)+geom_point()+scale_colour_brewer(palette="Set1",name="Category1")
ggplot(sm,aes(x=factor(month),y=price,group=CT1,fill=CT1))+geom_area(position="fill")+scale_fill_brewer(palette="Set1",name="Category1")

gn<-read.csv("GNI2010.csv")
library(portfolio)
map.market(id=gn$iso3,group = gn$continent,area = gn$population,color = gn$GNI-mean(gn$GNI),main="GNI",lab=c("id"=TRUE,"group"=TRUE))


#버블차트
aa<-ddply(sa2,~CT2,summarise,ta=sum(Amount),tp=sum(Price),fr=length(Amount),mp=mean(Price),cus=length(unique(ID)))
symbols(aa$mp,aa$fr,circles = aa$cus,inches = 0.5,bg="skyblue",fg="white")
text(aa$mp,aa$fr,aa$CT2,cex=0.5)

#히스토그램 
br<-read.csv("http://datasets.flowingdata.com/birth-rate.csv")
hist(br$X2008,20) #20은 구간의 갯수
br$X2008<-na.omit(br$X2008) #na값제거
hist(br2008,freq = FALSE) #비율로 
lines(density(br2008,adjust = 0.5),col=2) #선그리기,adjust 스무스 조정 

library(MASS)
attach(geyser)
de<-kde2d(waiting,duration,n=100)
image(de,xlab="waiting",ylab="duration")
contour(de,xlab="waiting",ylab="duration") #등고선으로 표현
persp3d(de,back="lines",col="skyblue",xlab="waiting",ylab="duration")#3차원 분포 그래프 


#히스토그램 행렬 
bre<-reshape(br,varying = list(2:50),direction = "long",idvar = "Country",v.names = "birth",timevar = "year",times=colnames(br)[-1])
library(lattice)
bre<-bre[bre$birth<132,] #이상치 제거 
h<-histogram(~birth|year, data=bre,layout=c(10,5))#행 10, 열5개의 히스토그램 행렬 
update(h,index.cond=list(c(41:50,31:40,21:30,11:20,1:10))) #행렬 순서 정

#히트맵
ba<-read.csv("http://datasets.flowingdata.com/ppg2008.csv")
rownames(ba)<-ba[,1]
ba<-ba[,-1]
ba<-as.matrix(ba)
heatmap(ba,scale = "column",Colv = NA)

#체르노프 페이스
library(aplpack)
faces(ba)

#별그림
cr<-read.csv("http://datasets.flowingdata.com/crimeRatesByState-formatted.csv")
rownames(cr)<-cr[,1]
cr<-cr[,-1]
st<-stars(cr,flip.labels = T)
stars(cr,flip.labels = FALSE,key.loc = c(16.1,1.8),draw.segments=TRUE)

#P4-1번
sa<-read.csv("sales.csv")
de<-read.csv("demo.csv")
mg<-merge(sa,de,by="ID")
library(plyr)
a<-ddply(mg,~CT2,summarise,ta=sum(Amount),tp=sum(Price),fr=length(Amount),mp=mean(Price),nc=length(unique(ID)),ma<-mean(Age))
hist(log(a$ta))
hist(log(a$tp))
hist(log(a$fr))
hist(log(a$mp))
hist(a$nc)
library(psych)

pairs.panels(log(a[,-1]))

aa<-a
rownames(aa)<-aa[,1]
aa<-aa[,-1]
aa<-as.matrix(aa)
heatmap(aa,scale = "column",Colv = NA)
s1<-stars(aa,flip.labels = F)
stars(aa,flip.labels = FALSE,key.loc = c(16.1,1.8),draw.segments=TRUE)



#P4-2번
b<-read.csv("baseball_2015.csv")
bb<-b[,-1]
library(lattice)
parallel(bb[,3:15],horizontal.axis=FALSE,col=1)


#회귀분석과제
library(devtools)
library(regbook)
a<-usedcars
par(mfrow=c(2,2))
y<-resid(lm(price~mileage+cc+automatic,a))
x1<-resid(lm(year~mileage+cc+automatic,a))
plot(y~x1,main="year")
abline(lm(y~x1))
grid()

y2<-resid(lm(price~year+cc+automatic,a))
x2<-resid(lm(mileage~year+cc+automatic,a))
plot(y2~x2,main="mileage")
abline(lm(y2~x2))
grid()

y3<-resid(lm(price~year+mileage+automatic,a))
x3<-resid(lm(cc~year+mileage+automatic,a))
plot(y3~x3,main="cc")
abline(lm(y3~x3))
grid()

y4<-resid(lm(price~year+mileage+cc,a))
x4<-resid(lm(automatic~year+mileage+cc,a))
plot(y4~x4,main="automatic")
abline(lm(y4~x4))
grid()

#회귀 연습문제 5.9

library(regbook)
a<-houseprice
fit<-lm(price~.,a)
plot(fit,which=4)#쿡의거리
cooks.distance(fit)
cooks.distance(fit)[cooks.distance(fit)>3.67/(27-5)] # 8,9,10,27이 계수 추청량에 영향을 준다.


plot(fit,which=5) # 내표준화 잔차대 지렛값 그림 
hatvalues(fit)
hatvalues(fit)[hatvalues(fit)>2*5/27] #9,27이 높은 지렛점일 가능성을 보이고 있다.


dffits(fit)    #dffits
plot(dffits(fit),ylab = 'DFFITS')
grid()
abline(h=2*sqrt(5/27),lty=3,col=2)
abline(h=-2*sqrt(5/27),lty=3,col=2)
identify(dffits(fit))

par(mfrow=c(2,2))   #dfbetas
par(mfrow=c(2,2))
dfbetas(fit)
d<-dfbetas(fit)
plot(d[,2],ylab="tax")
grid()
abline(h=2/sqrt(27),col=2,lty=3)
abline(h=-2/sqrt(27),col=2,lty=3)
identify(d[,2])

plot(d[,3],ylab="ground")
grid()
abline(h=2/sqrt(27),col=2,lty=3)
abline(h=-2/sqrt(27),col=2,lty=3)
identify(d[,3])

plot(d[,4],ylab="floor")
grid()
abline(h=2/sqrt(27),col=2,lty=3)
abline(h=-2/sqrt(27),col=2,lty=3)
identify(d[,4])


plot(d[,5],ylab="year")
grid()
abline(h=2/sqrt(27),col=2,lty=3)
abline(h=-2/sqrt(27),col=2,lty=3)
identify(d[,5])

#회귀 시험대비


pnorm(0)#표준정규 분포에서 누적분포함수값
pnorm(1.96)-pnorm(-1.96)#구간 값 
qnorm(0.95) #상위 0.05값
qnorm(0.05,lower.tail = FALSE)
dnorm(0) #확률밀도(질량)함수의 값
pnorm(-5/6)

library('regbook')
h<-hweight
m<-subset(h,gender=='M')
head(m)
hist(m$height)
hist(m$height,probability = TRUE)#세로축 비율 
lines(density(m$height)) #확률밀도함수 추가로 그리기
histf(m$height)#히스토그램 위에 정규분포의 pdf와 평활곡선 그려준다 
summary(m)
summary(m$height)#기본적인 측도계산(평균,사분위수,최대 최소)
range(m$height)#최대 최소값
quantile(m$height) #범위
quantile(m$height,probs = c(0,1)) #특정 분위수 
IQR(m$height)#사분위수 범위
boxplot(height~gender,h,horizontal=TRUE)#boxplot horizon은 눕혀서 그려라

#평균키 95%에 대한 신뢰구간
y<-m$height
n<-length(y)
mean(y)+qt(c(0.025,0.975),n-1)*sd(y)/sqrt(n)

t.test(m$height,mu=173,alternative = "two.sided") #양측검정
#정규성 검증 점들이 일직선에 있으면 정규성을 만족
qqnorm(m$height)
qqline(m$height)
#표본 상관계수
with(m,cor(height,weight))
cor(m$height,m$weight)
#상관계수 가설검정
cor.test(m$weight,m$height)

#2장 회귀분석

a<-aflength
head(a)
plot(a$forearm,a$foot)
plot(foot~forearm,a)
#예제2.2
x<-a$forearm
y<-a$foot
beta1<-cov(x,y)/cov(x,x)
beta<-mean(y)-beta1*mean(x)
lm(y~x,a)#lm 회귀분석에 필요한 거의 모든 통계값 계산(회귀계수)
fitted(lm(y~x,a))#적합값
resid(lm(y~x,a)) #잔차
plot(y~x,a)
abline(lm(y~x,a)) #추정된 회귀식 그리기
#예제2.3
summary(lm(y~x,a)) #잔차,회귀계수

#예제2.5
anova(lm(y~x,a))#분산 분석(반응변수,설명변수),귀무가석 베타원은 0 ,sst,ssr에대한정보  
summary(lm(y~x,a))# 결정계수,잔차표준오차, f검정 결과 
sum(rpredict(lm(y~x,a))^2) #press(예측잔차제곱함)

#예제2.6
summary(lm(y~x,a)) #점추정값,표준오차,t검정,p
confint(lm(y~x,a)) #95%신뢰구간 
#예제2.7(#각 관측값에 대한 신뢰구간 예측구간)
predict(lm(y~x,a),se.fit = TRUE,interval = 'confidence') #se=평균에 대한 표준오차,confidence = y의 평균에대한 신뢰구간
predict(lm(y~x,a),interval = 'prediction')#prediction y의 새로운값에 대한 예측구간
fitplot(lm(y~x,a)) #신뢰대,예측대

#예제2.8
par(mfrow=c(1,1))
plot(lm(y~x,a))
histf(rstandard(lm(y~x,a),boxplot=TRUE)) #표준화잔차 히스토그램과 상자그림 
#예제2.9
q<-quadratic
plot(y~x,q)
abline(lm(y~x,q))
with(q,lines(lowess(x,y),lty=2))
lm(y~x+I(x^2),q)#모형 수정
summary(lm(y~x+I(x^2),q))

#예제2.10
lmc=lm(y~-1+x,a)
summary(lmc)
confint(lmc)
anova(lmc)

#3장
u<-usedcars
pairs(usedcars)
c<-lm(price~.,u)
summary(c)
fitted(c)#예측값
resid(c) #잔차

#예제3.3(분산분석 귀무가설 =(베타원부터 p-1,회귀계수 다 0))
anova(c)#residuals잔차,나머지 ssr
summary(c)
sum((rpredict(c)^2))#예측 잔차제곱합(press)

#예제3.4
summary(c)#회귀계수가 0이라는 가설에 대한 t검정을 하여라
confint(c)
cc.lm<-lm(price~year+mileage+automatic,u)
summary(cc.lm)

#예제3.7
predict(c,interval = 'confidence')
predict(c,interval = 'prediction')
nw<-data.frame(year=60,mileage=100000,cc=2000,automatic=1) #새로운값 데이터프레임으로
nw
predict(c, newdata=nw, interval="confidence")#평균 신뢰구간
predict(c, newdata=nw, interval="prediction")# 예측값에 대한 예측 구간
predict(c, newdata=nw, interval="confidence", se.fit=TRUE) #평균 추정값에 대한  표준오차 
#예제 3.8
par(mfrow=c(2,2))
plot(c) #상단 왼쪽 
res<-rstandard(c)
plot(res~year,u) #표준화 잔차대 설병변수들의 산점도 
abline(h=0,lty=3)
plot(price~fitted(c),u) #관측값대 예측값 산점도
abline(0,1,lty=3)
library(lattice)
rfs(c) #rf그림 
summary(c)

#예제4.2
anova(c) #추가제곱합
drop1(c, test="F")  # rss편제곱합의 크기, 293332-255538 ,편제곱합의 크기(설명변수의 중요도) 
#440번가설 p값>0.05보다 크면 귀무가설 베타는=0 기각x
##예제 4.3
mod0 <- lm(price ~ year + mileage + automatic, usedcars) #model 2개만들어서 f검정 가능 
mod1 <- lm(price ~ year + mileage + cc + automatic, usedcars) 
anova(mod0, mod1)
mod0 <- lm(price ~ year + mileage, usedcars) #귀무가설 모형
mod1 <- lm(price ~ year + mileage + cc + automatic, usedcars) #대립가설 모형  
anova(mod0, mod1)                

fit<-lm(price~year,usedcars)
summary(fit)
#예제4.5(편회귀 그림)
fit <- lm(price ~ year+mileage,usedcars)
coef(summary(fit))# 편회귀 계수만 출ㄹ
y.x1 <- resid(lm(price ~ year, usedcars))
x2.x1 <- resid(lm(mileage ~ year, usedcars))
plot(y.x1 ~ x2.x1)
abline(lm(y.x1 ~ x2.x1))
grid()
coef(lm(y.x1 ~ x2.x1))
y.x2<-resid(lm(price~mileage,u))
x1.x2<-resid(lm(year~mileage,u))
plot(x1.x2,y.x2)
grid()
abline(lm(y.x2~x1.x2))
#편회귀 그림에서 점들이 회귀 직선에 가까이 있으면 모형의 적합도가 높다
y.x1<-resid(lm(price~mileage+cc+automatic,u))
k.x1<-resid(lm(year~mileage+cc+automatic,u))
plot(k.x1,y.x1)
abline(lm(y.x1~k.x1))
#적합 회귀식 과 lowess평활곡선 그리기 
quadratic
plot(y ~ x, quadratic)
quadratic.lm1 <- lm(y ~ x, quadratic)
abline(quadratic.lm1)
with(quadratic, lines(lowess(x,y), lty=2))
#예제 4.7
expdata1
e<-lm(y~x1,expdata1)
anova(e)
et<-lm(y~factor(x1),expdata1) #순오차제곱 
anova(et)
anova(e,et) #오차제곱합은 순오차제곱합과 적합결여제곱합 

e1<-lm(y~x1+x2,expdata1)
e2<-lm(y~factor(x1)+factor(x2),expdata1)
anova(e1,e2)

bug
par(mfrow=c(1,2))
plot(y ~ time, bug)
bug <- within(bug, logy <- log(y))
fit <- lm(logy ~ time, bug)
plot(logy ~ time, bug)
abline(fit)
summary(fit)
#예제4.9(분산 안정화변환 )
r<-restaurant
plot(Y ~ X, restaurant)
a <-lm(Y ~ X, restaurant)
abline(a) 

par(mfrow=c(2,2))
plot(a)

meanvar(Y ~ X, restaurant) # 평균과 분산 계산 구해준
ym<-meanvar(Y ~ X, restaurant)
plot(ym,sd~mean+I(mean^2))
b<-lm(1/Y~X+I(X^2),r)
summary(b)
c<-lm(1/Y~X+I(X^2)+I(X^3),r) #결정계수가 더높다 계속 이런식으로 하나씩 추가해준다 
summary(c)
#box-cox변환 
wool
boxcox(lm(cycle~length+amplitude+load,wool)) #로그변환 필요
qqnorm(wool$cycle)
qqline(wool$cycle)
b<-within(wool,logy<-log(wool$cycle))
qqnorm(b$logy)
qqline(b$logy)
al<-lm(logy~length+amplitude+load,b)
summary(al)
par(mfrow=c(2,2))
plot(al)
#예제4.12(표준화 회귀계수)
a<-houseprice
la<-lm(price~.,a)
summary(la)
stdcoef(la) #표준화 회귀계수
summary(lmbeta(la)) #표준화 회귀계수뿐만 아니라 회귀분석도

#예제4.14
a<-hald
a
fit<-lm(y~.,a)
summary(fit)
r<-cor(a[2:5]) #다중공선성 유무판단하기위해서 설명변수들의 상관계수
r
summary(vif(fit)) #vif 분산팽창인수(10보다크면 다중공선성) ei 고유값(0에 가까우면 하나의 선형종속관계 존재 ),cond조건지수(클수록 다중공선성)

z<-lm(y~+x1+x3+x4,a)
summary(z)
summary(vif(z))

#5장(지렛점,특이점)
#예제5.1
a<-usedcars
fit<-lm(price~.,a)
resid(fit)
rstandard(fit)#내표준화잔차(+-2넘으면 특이점)
rstudent(fit)  ('')
length(a)
n<-length(resid(fit)) 
hatvalues(fit) #지렛값(2p/n보다 크면 특이점,p는 회귀계수 갯수 )

influence.measures(fit) #모든 측도다 
cooks.distance(fit)#3.67/(n-p)
covratio(fit) # |coverat-1|>=3p/n
dfbetas(fit) #2/루트n
diffits(fit) #2*루트 p/n

#예제5.4(독립성 판정)
install.packages("lmtest") 
library(lmtest)   
steel
fit<-lm( steelprice~machineprice,steel)
summary(fit)
plot(fit)
plot(steelprice~machineprice,steel)
abline(fit)
lag.plot(resid(fit),do.lines = FALSE) #시차그림(독립이면 랜덤,그렇치않으면 선형관계)
dwtest(fit) #더비 왓슨-독립성 판정
e<-resid(fit)
ar(e,aic = FALSE,order.max = 1) #일차 자기상관계수

#6장(상관계수,산점도,회귀분석,잔차,특이점, 자기상관관계)
h<-houseprice
cor(h)
cor.test(h$price,h$tax)
pairs(h)
plot(h$price~h$year,h)
plot(h$price~h$tax,h)
fit<-lm(price~.,h)
summary(fit)
resid(fit)
rstandard(fit)
rstudent(fit)
hatvalues(fit) 
par(mfrow=c(2,2))
plot(fit)
lag.plot(resid(fit),do.lines = FALSE)
library(lmtest)
dwtest(fit)
library(leaps)
h.rgs<-regsubsets(price~.,h,nbest = 6) #가능한 모형 6개까지
summaryf(h.rgs) #잔차제곱합, 결정계수,조정결정계수(큰값이 최적),멜로우즈(최소가 적합),bic(최소가 최적) 

press(fit) #예측도
press(h.rgs) #press최소가 최적모형 

model0<-lm(price~1,h)
add1(model0,scope=~tax+ground+floor+year,test='F')#('F가 큰게 모형에 포함)
mo<-lm(price~floor,h) #f값 가장 큰 변수 추가
add1(mo,scope=~tax+ground+floor+year,test='F')
mo2 <- update(mo, . ~ . +tax)
mo2
drop1(mo2, test="F")
add1(mo2, scope= ~tax+ground+floor+year, test="F")
step(model0,scope=~tax+ground+floor+year,direction = "both") #add1,drop1한번에 해준
step(model0,scope=~tax+ground+floor+year,direction = "forward")
#모형 확인 
fit1<-lm(price~tax+floor,h)
summary(fit1)
rstandard(fit1)
rstudent(fit1)
press(fit1)
rpredict(fit1)





library(MASS)
b<-Boston
b
fit<-lm(medv~lstat,b)
summary(fit)
confint(fit)


#다변량 통계분석 
a<-read.csv("Advertising.csv")
a<-a[,-1]
library(psych)
pairs.panels(a)
fit <-lm(Sales ~ TV, a)
summary(fit)
str(a)
fit2<-lm(Sales~.,a)          
summary(fit2)
anova(fit,fit2)
fit3<-lm(Sales~TV+Radio,a)
anova(fit3,fit2)

step(lm(Sales~1,a),scope = ~TV+Radio+Newspaper,direction = "forward")
step(lm(Sales~.,a),direction = "backward")
step(lm(Sales~1,a),scope = ~TV+Radio+Newspaper,direction = "both")

library(leaps)
sub<-regsubsets(Sales~.,a,nbest = 3) 
plot(sub)  
fit4<-lm(Sales~TV+Radio,a)
predict(fit4,data.frame(TV=100,Radio=20),interval = "confidence")
predict(fit4,data.frame(TV=100,Radio=20),interval = "predict") 
#2번 
str(b)
fit.all<-lm(medv~.,b)
summary(fit.all)
anova(fit.all)
fit.c<-lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,b)
anova(fit.c,fit.all)
library(car)
install.packages("car")
vif(fit.all)
step(lm(medv~1,b),scope=~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,direction = "forward")
step(lm(medv~.,b),direction="backward")
step(lm(medv~1,b),scope=~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,direction = "both")
library(leaps)
sub<-regsubsets(medv~.,b) 
plot(sub,scale="bic")
plot(sub,scale="adjr")

#3번 
library(ISLR)
ca<-Carseats
str(ca)
fit.ca<-lm(Sales~ShelveLoc ,ca)
summary(fit.ca)
head(model.matrix(fit.ca))

fit.ca2 = lm(Sales~ShelveLoc ,ca, contrasts = list(ShelveLoc=contr.sum))
head(model.matrix(fit.ca2))
head(ca)
summary(fit.ca2)

fit.cc<-lm(Sales~. ,ca)
summary(fit.cc)
library(car)
library(leaps)
sub1<-regsubsets(Sales~.,ca) 
par(mfrow=c(1,2))
plot(sub1,scale="bic")
plot(sub1,scale="adjr2")

fit.f<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age,ca)

fit.f2<-lm(Sales~CompPrice+Income+Advertising+Price+Age,ca)
vif(fit.f2)
plot(fit.f)
step(lm(Sales~.,ca),direction="backward")
step(lm(Sales~1,ca),scope=~CompPrice+Income +Advertising+Population +Price +ShelveLoc+Age +Education +Urban +US,direction = "forward")

fit.ff<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+Income*Advertising,ca) 
summary(fit.ff)
par(mfrow=c(2,2))
plot(fit.ff)
summary(ca)
predict(fit.ff,data.frame(CompPrice=125,Income=68.66,Advertising=6.635,Price=115.8,ShelveLoc="Medium",Age=53.35),interval = "prediction")


#1번 
library(MASS)
b<-Boston
fit<-lm(medv~lstat,b)
summary(fit)
plot(medv~lstat,b)
abline(fit,col=2)
confint(fit) 

#2번
str(b)
fit.all<-lm(medv~.,b)
summary(fit.all)
anova(fit.all)
fit.c<-lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,b)
anova(fit.c,fit.all)
library(car)
install.packages("car")
vif(fit.all)
step(lm(medv~1,b),scope=~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,direction = "forward")
step(lm(medv~.,b),direction="backward")
step(lm(medv~1,b),scope=~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,direction = "both")
library(leaps)
sub<-regsubsets(medv~.,b) 
plot(sub,scale="bic")
plot(sub,scale="adjr")
 
#3번 
library(ISLR)
ca<-Carseats
str(ca)
fit.ca<-lm(Sales~ShelveLoc ,ca)
summary(fit.ca)
head(model.matrix(fit.ca))

fit.ca2 = lm(Sales~ShelveLoc ,ca, contrasts = list(ShelveLoc=contr.sum))
head(model.matrix(fit.ca2))
head(ca)
summary(fit.ca2) 

fit.ca3<-lm(Sales~CompPrice+Income+Advertising+Population+Price+ShelveLoc+Age+Education+Urban+US+Income*Advertising+Price*Age,ca)
summary(fit.ca3)

fit.cc<-lm(Sales~. ,ca)
summary(fit.cc)
vif(fit.cc)
sub1<-regsubsets(Sales~.,ca) 
plot(sub1,scale="bic")
plot(sub1,scale="adjr2")

fit.f<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+US,ca)
str(ca)
plot(fit.f)
library(psych)
pairs.panels(ca)
plot(ca)
step(lm(Sales~.,ca),direction="backward")
step(lm(Sales~1,ca),scope=~CompPrice+Income +Advertising+Population +Price +ShelveLoc+Age +Education +Urban +US,direction = "forward")
str(ca)

fit.f<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age,ca)
fit.f2<-lm(Sales~CompPrice+Income+Advertising+Price+Age,ca)
vif(fit.f2)
fit.ff<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+Income*Advertising,ca) 




#데이터 마이닝
#ch1
train <- read.csv("pepTrainSet.csv", stringsAsFactors=F)
train <- subset(train, select=-c(id))
test <- read.csv("pepTestSet.csv", stringsAsFactors=F)
newd <- read.csv("pepNewCustomers.csv", stringsAsFactors=F)

train$pep <- factor(train$pep)
test$pep <- factor(test$pep)


install.packages("caret")
install.packages("ROCR")
install.packages("C50")

library(caret)
library(ROCR)
library(C50)

c5_options <- C5.0Control(winnow = FALSE, noGlobalPruning = FALSE)
c5_model <- C5.0(pep ~ ., data=train, control=c5_options, rules=FALSE)
summary(c5_model)
plot(c5_model)

lm_model <- glm(pep ~ ., data=train, family = binomial)
summary(lm_model)

test$c5_pred <- predict(c5_model, test, type="class")
test$c5_pred_prob <- predict(c5_model, test, type="prob")


install.packages("e1071")
library(e1071)
confusionMatrix(test$c5_pred, test$pep)


test$lm_pred <- ifelse(predict(lm_model, test, type="response") > 0.5, "YES", "NO")
test$lm_pred_prob <- predict(lm_model, test, type="response")
confusionMatrix(test$lm_pred, test$pep)

c5_pred <- prediction(test$c5_pred_prob[, "YES"], test$pep)
c5_model.perf <- performance(c5_pred, "tpr", "fpr")

lm_pred <- prediction(test$lm_pred_prob, test$pep)
lm_model.perf <- performance(lm_pred, "tpr", "fpr")

plot(c5_model.perf, col="red")
plot(lm_model.perf, col="blue", add=T)
legend(0.6, 0.6, c("C5","LM"), cex=0.9, col=c("red", "blue"), lty=1)

newd$c5_pred <- predict(c5_model, newd, type="class")
newd$c5_pred_prob <- predict(c5_model, newd, type="prob")
target <- subset(newd, c5_pred=="YES" & c5_pred_prob[, "YES"] > 0.8)
write.csv(target[order(target$c5_pred_prob[, "YES"], decreasing=T), ], "dm_target.csv", row.names=FALSE)




#ch.2
cb1 <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
str(cb1)
cb1$반품여부 <- factor(cb1$반품여부)
set.seed(1)
inTrain1 <- createDataPartition(y=cb1$반품여부, p=0.6, list=FALSE)
cb.train1<- cb1[inTrain,]
cb.test1 <- cb1[-inTrain,]
dim(cb.train); dim(cb.test)


c5_options <- C5.0Control(winnow = FALSE, noGlobalPruning = FALSE)
c5_model <- C5.0(반품여부 ~ 성별+나이+구매금액+출연자, data=cb.train1, control=c5_options, rules=FALSE)
summary(c5_model)
plot(c5_model)

cb.test1$c5_pred <- predict(c5_model, cb.test1,type="class")
cb.test1$c5_pred_prob <- predict(c5_model, cb.test1, type="prob")
head(cb.test1)


confusionMatrix(cb.test1$c5_pred, cb.test1$반품여부)

c5_pred <- prediction(cb.test1$c5_pred_prob[,2],cb.test$반품여부)
c5_model.perf1 <- performance(c5_pred, "tpr", "fpr") 
c5_model.perf2 <- performance(c5_pred, "lift", "rpp")
plot(c5_model.perf1, colorize=TRUE)
plot(c5_model.perf2, colorize=TRUE)
performance(c5_pred, "auc")@y.values[[1]]

install.packages("Epi")
library(Epi)
ROC(form=cb.test$반품여부~c5_pred_prob[,2], data=cb.test, plot="ROC")

#ch.3
install.packages("randomForest")
library(randomForest)


cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
cb$반품여부 <- factor(cb$반품여부)

set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]

set.seed(123)
rf_model <- randomForest(반품여부 ~ .-ID, data=cb.train, ntree=50, mtry=2)
rf_model

plot(rf_model, main="random Forest model")
importance(rf_model)
varImpPlot(rf_model)
cb.test$rf_pred <- predict(rf_model, cb.test, type="response")
confusionMatrix(cb.test$rf_pred, cb.test$반품여부)

cb.test$rf_pred_prob <- predict(rf_model, cb.test, type="prob")
rf_pred <- prediction(cb.test$rf_pred_prob[,2],cb.test$반품여부)
rf_model.perf1 <- performance(rf_pred, "tpr", "fpr") 
rf_model.perf2 <- performance(rf_pred, "lift", "rpp") 
plot(rf_model.perf1, colorize=TRUE); abline(a=0, b=1, lty=3)
plot(rf_model.perf2, colorize=TRUE); abline(v=0.4, lty=3)
performance(rf_pred, "auc")@y.values[[1]] 

install.packages("e1071")
library(e1071)
svm_model <- svm(반품여부~성별+나이+구매금액+출연자, data=cb.train, cost=100, gamma=1, probability = TRUE)
summary(svm_model)
plot(svm_model, data=cb.train, 구매금액~나이)

cb.test$svm_pred <- predict(svm_model, cb.test)
confusionMatrix(cb.test$rf_pred, cb.test$반품여부)

postResample(cb.test$svm_pred, cb.test$반품여부)


cb.test$svm_pred_prob <- attr(predict(svm_model, cb.test, probability = TRUE), "probabilities")[,2]
svm_pred <- prediction(cb.test$svm_pred_prob, cb.test$반품여부)
svm_model.perf1 <- performance(svm_pred, "tpr", "fpr")
svm_model.perf2 <- performance(svm_pred, "lift", "rpp") 
plot(svm_model.perf1, colorize=TRUE); abline(a=0, b=1, lty=3)
plot(svm_model.perf2, colorize=TRUE); abline(v=0.4, lty=3)
performance(svm_pred, "auc")@y.values[[1]]
set.seed(123)
tune.svm(반품여부~성별+나이+구매금액+출연자, data=cb.train, gamma=seq(.5, .9, by=.1), cost=seq(100,1000, by=100))

set.seed(1)
flds <- createFolds(cb$반품여부, k=5, list=TRUE, returnTrain=FALSE)
str(flds)

experiment <- function(train, test, m) {
  rf <- randomForest(반품여부 ~ .-ID, data=train, ntree=50)
  rf_pred <- predict(rf, test, type="response")
  m$acc = c(m$acc, confusionMatrix(rf_pred, test$반품여부)$overall[1])
  rf_pred_prob <- predict(rf, test, type="prob")
  rf_pred <- prediction(rf_pred_prob[,2], cb.test$반품여부)
  m$auc = c(m$auc, performance(rf_pred, "auc")@y.values[[1]])
  return(m) 
}

measure = list()
for(i in 1:5){
  inTest <- flds[[i]]
  cb.test <- cb[inTest, ]
  cb.train <- cb[-inTest, ]
  measure = experiment(cb.train, cb.test, measure) 
}
measure 
mean(measure$acc); sd(measure$acc)
mean(measure$auc); sd(measure$auc)



#데이터 마이닝 과제

library("xgboost")
library(caret)
library(ROCR)

cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
head(cb)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
#학습용 데이터와 검증데이터 나누기
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]

cb.train_m<-cb.train[,2:6]
cb.train_m<-cb.train_m[,1:4]
cb.test_m<-cb.test[,2:6]
cb.test_m<-cb.test_m[,1:4]

matrix_train <- apply(cb.train_m, 2, function(x) as.numeric(as.character(x)))
matrix_test <- apply(cb.test_m, 2, function(x) as.numeric(as.character(x)))

trla<-as.numeric(cb.train$반품여부)
tela<-as.numeric(cb.test$반품여부)

xgb_train_matrix <- xgb.DMatrix(data = as.matrix(matrix_train), label = trla)
xgb_test_matrix <- xgb.DMatrix(data = as.matrix(matrix_test), label = tela)

watchlist <- list(train = xgb_train_matrix, test = xgb_test_matrix)
label <- getinfo(xgb_test_matrix, "label")
param <- list("objective" = "binary:logistic")

#cross validation to evaluate the error rate
xgb.cv(param = param, 
       data = xgb_train_matrix, 
       nfold = 3,
       label = getinfo(xgb_train_matrix, "label"),
       nrounds = 6)

#training with gbtree
bst_1 <- xgb.train(data = xgb_train_matrix, 
                   label = getinfo(xgb_train_matrix, "label"),
                   max.depth = 3, 
                   eta = 1, 
                   nthread = 2, 
                   nround = 50, # number of trees used for model building
                   watchlist = watchlist, 
                   objective = "binary:logistic")

features = colnames(matrix_train)
importance_matrix_1 <- xgb.importance(features, model = bst_1)
print(importance_matrix_1)
xgb.ggplot.importance(importance_matrix_1) +theme_minimal()

pred_1 <- predict(bst_1, xgb_test_matrix)
result_1 <- data.frame(ID = rownames(cb.test),
                       outcome = cb.test$반품여부, 
                       label = label, 
                       prediction_p_refund = round(pred_1, digits = 2),
                       prediction = as.integer(pred_1 > 0.5),
                       prediction_eval = ifelse(as.integer(pred_1 > 0.5) != label, "wrong", "correct"))
result_1

err <- as.numeric(sum(as.integer(pred_1 > 0.5) != label))/length(label)
print(paste("test-error =", round(err, digits = 2)))

#confusionMatrix 
confusionMatrix(result_1$prediction,cb.test$반품여부)

xg_pred<-prediction(result_1$prediction_p_refund,cb.test$반품여부)
xg_model.perf1 <- performance(xg_pred, "tpr", "fpr")
xg_model.perf2 <- performance(xg_pred, "lift", "rpp")
plot(xg_model.perf1, colorize=TRUE)
plot(xg_model.perf2, colorize=TRUE)
performance(xg_pred, "auc")@y.values[[1]]
library(Epi)
ROC(form=cb.test$반품여부~prediction_p_refund, data=result_1, plot="ROC")

#다변량 중간고사 준비
ad<-read.csv("Advertising.csv")
ad<-ad[,-1]
library(psych)
pairs.panels(ad)
fit<-lm(Sales~TV,ad)
summary(fit)
residuals(fit) #잔차
confint(fit)
confint(fit,level = 0.99)
fit.c<-lm(dist~speed,cars) #자동차 자료
summary(fit.c)  
plot(cars$speed,cars$dist)
fit.c<-lm(dist~0+speed,cars)#원점을 지나는 회귀선


plot(Sales~TV,ad)
abline(fit,col=2,lwd=2)

#부분f검정 
fit2<-lm(Sales~.,ad)
summary(fit2)
fit3<-lm(Sales~TV+Radio,ad)
anova(fit3,fit2)

#전진,후진,혼합 
step(lm(Sales~1,ad),scope = ~TV+Radio+Newspaper,direction = "forward")
step(lm(Sales~.,ad),direction = "backward")
step(lm(Sales~1,ad),scope = ~TV+Radio+Newspaper,direction = "both")


install.packages("leaps")
library(leaps)
sub<-regsubsets(Sales~.,ad) #nbest=설명변수 3개를 고르겠다
plot(sub,scale="bic")

fit4<-lm(Sales~TV+Radio,ad)
predict(fit4,data.frame(TV=c(100,130,150),Radio=c(20,10,5)),interval="prediction")

#질적변수 코딩 
install.packages("ISLR")
library("ISLR")
cr<-read.csv("Credit.csv")
head(cr)
cr<-cr[,-1]
library(psych)
pairs.panels(cr)

boxplot(Balance~Gender,cr)
boxplot(Balance~Student,cr)
boxplot(Balance~Ethnicity,cr)

fit<-lm(Balance~Gender,cr)
summary(fit)
str(cr)
cr$Gender<-relevel(cr$Gender,ref="Female")#레퍼런스 바꿔주기  


fit1<-lm(Balance~Gender,cr,contrasts = list(Gender=contr.sum))
summary(fit1)
head(model.matrix(fit1))
head(cr)

fit2<-lm(Balance~Ethnicity,cr)
fit3<-lm(Balance~Ethnicity,cr,contrasts = list(Ethnicity=contr.sum))
summary(fit3)
head(cr$Ethnicity,10)
head(model.matrix(fit3),10)

cr$Ethnicity<-relevel(cr$Ethnicity,ref="Asian")
str(cr)
fit5<-lm(Sales~TV+Radio+TV*Radio,ad)
summary(fit5)

fit6<-lm(Balance~Income+Student+Income*Student,cr)
summary(fit6)


a<-read.csv("Auto.csv")
plot(mpg~horsepower,a)
head(a)
plot(lm(mpg~horsepower,a))
b<-lm(mpg~horsepower,a)
fit7<-lm(mpg~horsepower+I(horsepower^2),a)
summary(fit7)
plot(fit7)

plot(fit7$model$horsepower,residuals(fit7))


head(a)
model3<-lm(mpg~.-name,a)
library(car)
vif(model3)

head(cr)
plot(Rating~Limit,cr,main="collinearity")
fit8<-lm(Balance~Age+Rating+Limit,cr)
vif(fit8)


#연습문제
#1
library(MASS)
b<-Boston
head(b)
fit<-lm(medv~lstat,b)
summary(fit)

plot(medv~lstat,b)
abline(fit,col=2,lwd=2)

confint(fit)
summary(fit)

#2
fit1<-lm(medv~.,b)
summary(fit1)
fit2<-lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,b)
anova(fit2,fit1)
library(car)
vif(fit1)
step(lm(medv~1,b),scope =~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat ,direction = "forward")
step(lm(medv~.,b),direction = "backward")
step(lm(medv~1,b),scope =~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat ,direction = "both")

library(leaps)
sub<-regsubsets(medv~.,b,nvmax = 13) #nvmax=모형이 가질수 있는 최대 설명변수의 갯수  
plot(sub,scale = "bic")

#3
library(ISLR)
ca<-Carseats
head(ca)
fit<-lm(Sales~.,ca)
summary(fit)
head(model.matrix(fit))
head(ca)

fit<-lm(Sales~.,ca,contrasts = list(ShelveLoc=contr.sum))
summary(fit)
head(model.matrix(fit))
head(ca)

fit<-lm(Sales~.+Income*Advertising+Price*Age,ca)
summary(fit)

pairs.panels(ca)
step(lm(Sales~.,ca),direction = "backward")
sub<-regsubsets(Sales~.,ca,nvmax =10 )
plot(sub,scale = "adjr2")
plot(sub,scale = "bic")


vif(fit1)
fit2<-lm(Sales~CompPrice+Income+Advertising+Price+Age,ca)
vif(fit2)
fitf<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+CompPrice*Income+CompPrice*Advertising
+CompPrice*Price+CompPrice*ShelveLoc+CompPrice*Age
+Income*Advertising+Income*Price+Income*ShelveLoc+Income*Age
+Advertising*Price+Advertising*ShelveLoc+Advertising*Age
+Price*ShelveLoc+Price*Age
+ShelveLoc*Age,ca)
summary(fitf)
fitf<-lm(Sales~CompPrice+Income+Advertising+Price+ShelveLoc+Age+Income*Advertising,ca)
par(mfrow=c(2,2))
plot(fitf)

summary(ca)
predict(fitf,data.frame(CompPrice=125,Income=68.66,Advertising=6.635,Price=115.8,ShelveLoc="Medium",Age=53.32),interval = "prediction")

plot(fitf$residuals)


install.packages("dvetools")
library(dvtools)

#4.13
library("ISLR")
glm.fit<-glm(default~balance,Default,family = 'binomial')
summary(glm.fit)
glm.fit2<-glm(default~balance+income+student,Default,family = "binomial")
summary(glm.fit2)
exp(-6.468e-01 )
glm.fit3<-glm(default~student,Default,family = 'binomial')
summary(glm.fit3)
exp(0.40489)
boxplot(balance~student,Default,ylab="balance")

qnorm(0.975) #95% z값
exp(-6.468e-01 +1.96*2.363e-01) #학생일 경우 파산확률 신뢰구간 (베타)
exp(-6.468e-01 -1.96*2.363e-01)
anova(glm.fit2,glm.fit3,test="LRT")  #chisq랑 같다 
anova(glm.fit2,glm.fit3,test="Chisq")

#practice2 

#1-a
d<-read.csv("dengue.csv")
str(d)
head(d)
d$DENGUE<--(d$DENGUE-2)
d$DENGUE<-factor(d$DENGUE)
d$MOSNET<-factor(d$MOSNET)
d$SECTOR<-factor(d$SECTOR)
glm.d<-glm(DENGUE~AGE+MOSNET+SECTOR,d,family="binomial")
summary(glm.d)
#1-b
exp(0.333525 )
exp(0.333525 +1.96*1.271833)
exp(0.333525 -1.96*1.271833)
#1-c
glm.d2<-glm(DENGUE~AGE+SECTOR,d,family="binomial")
anova(glm.d2,glm.d,test="LRT")
summary(glm.d2)

#1-d
exp(0.02403)
exp(5*0.02403)
0.00901
exp(0.02403*5+(0.00901*1.96) )
exp(0.02403*5-(0.00901*1.96) )


co<-read.csv("coupon.csv")
str(co)
glm.c<-glm(cbind(N_redeemed,N-N_redeemed)~Price_reduc,co,family="binomial")
summary(glm.c)
predict(glm.c,data.frame(Price_reduc=c(5,10,15,20)),type="response") #5,15,20,25일떄 사용할 확률 
pred<-data.frame(default=Default$default,fit=glm.fit2$fitted)
head(pred)
xtabs(~pred$default +(pred$fit > 0.5))
xtabs(~pred$default+(pred$fit>0.5))
(40+128)/(9627+40+228+105) #에러율

library(ROCR)
predob<-prediction(pred$fit,pred$default) #
?prediction
plot(performance(predob,"tpr","fpr")) #roc
plot(performance(predob,"err")) 



#practice 2번문제 

#2-a
library(ISLR)
s<-Smarket
head(s)
str(s)

glm.s<-glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,s,family = "binomial")
summary(glm.s)

#2-b
exp(-0.126000)
exp(-0.073074)
exp(-0.042301)
exp(0.011085)
exp(0.009359)
exp(0.010313)
exp(0.135441 )

#2-c
pred<-data.frame(direction=s$Direction,fit=glm.s$fitted)
xtabs(~pred$direction +(pred$fit > 0.5))

507/(141+507)
145/(145+457)
(457+141)/(145+457+141+507)

#2-d
library(ROCR)
predob<-prediction(pred$fit,pred$direction)
plot(performance(predob,"tpr","fpr"))

performance(predob, "auc")@y.values[[1]] #roc커브 면적 

#2-e
plot(performance(predob,"err")) 

#데마

library(data.table)

ck.train <- fread("train_clickstreams.tab")
pf <- read.csv("train_profiles.csv")
pf$gender <- substr(pf$GROUP,1,1)
pf$ge <- ifelse(pf$gender == "M",1,0) 
pf$gender <- NULL
library(reshape)
events <- unique(ck.train[,list(CUS_ID,ACT_NM)],by=NULL)
events$count = 1

ACTNM1 <- cast(events, CUS_ID ~ ACT_NM, sum, value = "count")
train <- merge(pf,ACTNM1,"CUS_ID")
library(caret)
inTrain <- createDataPartition(y=train$GROUP, p=0.5, list=FALSE)
train5 <- train[inTrain,]
test5 <- train[-inTrain,]

train6 <- train5[-3:-1]
test6 <- test5[-3:-1]


#5.4 다변량 
install.packages("MVA")
library(MVA)
summary(heptathlon)
h<-heptathlon
library(psych)
pairs.panels(h[,1:7])
h$hurdles <-with(h,max(hurdles)-hurdles) #잘할수록 기록이 크게 나오도록  바꾸기 
h$run200m<-with(h,max(run200m)-run200m)
h$run800m<-with(h,max(run800m)-run800m)
pairs.panels(h[,1:7])

h[h$highjump==min(h$highjump),]
h2<-h[h$highjump!=min(h$highjump),] #최소값 제거
pairs.panels(h2[,1:7])

pca1<-prcomp(h2[,1:7],scale. = T)
pca1
summary(pca1)
plot(pca1,type="l")

var(pca1$x[,1])
2.0793^2/7
plot(pca1$x[,1],h2$score)
cor(pca1$x[,1],h2$score)

biplot(pca1,xlim=c(-0.6,0.6)) #1번째와 2번째 주성분 비교 ,rownames이 있다.#빨간선(원래변수)와 pc축이 평행할수록 관계가 높다 ,음수이기때문에 음의방향을 가리킨다.막대의 길이 원래변수의 분산의 크기  
plot(pca1$x[,1],pca1$x[,2]) 
pca1

biplot(pca1,choice=c(1,3))

pca1<-prcomp(h2[,1:7]) #scale x
summary(pca1)
biplot(pca1)

#practice3
#1
bu<-read.csv("bulls.csv")
buu<-bu
bu<-bu[,-1]
bup<-prcomp(bu[,-1],scale=T)
bup
str(bup)
pairs.panels(bu[,-1])

#2
summary(bup)
plot(bup,type="l")


#3
plot(buu$SalePr,bup$x[,1])
cor(bup$x[,1],buu$SalePr)

plot(buu$SalePr,bup$x[,2])
cor(bup$x[,2],buu$SalePr)

plot(buu$SalePr,bup$x[,3])
cor(bup$x[,3],buu$SalePr)


plot(buu$SalePr,bup$x[,4])
cor(bup$x[,4],buu$SalePr)


plot(buu$SalePr,bup$x[,5])
cor(bup$x[,5],buu$SalePr)


plot(buu$SalePr,bup$x[,6])
cor(bup$x[,6],buu$SalePr)


plot(buu$SalePr,bup$x[,7])
cor(bup$x[,7],buu$SalePr)

bup$rotation[,c(1,2)]


#4
biplot(bup,xlim=c(-0.4,0.4))
pairs.panels(bu[,-1])

#5

b<-bup$x[,c(1,2)]
buu[,1]<-as.factor(buu[,1])
b<-as.data.frame(b)
b$Breed<-buu[,1]
head(b)
library(ggplot2)
g<-ggplot(b,aes(x=PC1,y=PC2,group=factor(Breed),color=factor(Breed),shape=factor(Breed)))+geom_point(,size=3)


g



#6
qqnorm(b$PC1)
qqline(b$PC1)

#5.11 다변량 
ap<-read.table("Applicant.txt",header = T)
str(ap)
summary(ap)
aps<-scale(ap)
cov(aps)
fal<-factanal(aps,4)
print(fal)
rownames(ap)<-ap$ID
ap<-ap[,-1] #id 제거 
str(ap)
aps<-scale(ap)
cov(aps)
fal<-factanal(aps,4)
print(fal,digits = 2)
print(fal,digits = 2,sort=T)
0.92^2+0.14^2
ld<-fal$loadings
plot(ld,type = "n")
aps<-data.frame(aps)
text(ld,labels = names(aps))

ap<-read.table("Applicant.txt",header = T)
rownames(ap)<-ap$ID
ap<-ap[,-1]
str(ap)
summary(ap)
aps<-scale(ap)
cov(aps)
fal<-factanal(aps,4)
print(fal,digits = 2,sort=T) #sort 펙터에 대해서 로딩값이 큰 순서대로 정렬
aps<-data.frame(aps)
ld<-fal$loadings
plot(ld,type = "n")
text(ld,labels = names(aps))
fa2<-factanal(aps,2)
print(fa2.digits=2,sort=T)
fa2<-factanal(aps,2)
print(fa2.digits=2,sort=T)
print(fa,digits = 2,sort=T)
print(fa2,digits = 2,sort=T)
factanal(aps,1)
factanal(aps,2)
print(factanal(aps,6))

s<-read.csv("Stock_price.csv")
str(s)
head(s)
tail(s)
s<-s[,-1]
ss<-scale(s)
factanal(ss,1)
factanal(ss,2)


sp<-prcomp(s,scale. = T)
summary(sp)
plot(sp,type="l")
scree.plot(sp)
head(ap)
head(aps)
fas<-factanal(s,1)
print(fas)

str(s)
factanal(s,2)
factanal(s,1)
factanal(s,2)
factanal(s,3)
print(fas,digits = 2,sort=T)
fas<-factanal(s,2)
print(fas,digits = 2,sort=T)
head(s)
print(fas,digits = 2,sort=T)

0.76^2
0.67^2+0.11^2
0.82^2+0.23^2
0.11^2+0.99^2
0.11^2+0.68^2
factanal(s,1)
factanal(s,2)
print(fas,digits = 2,sort=T)
l<-fas$loadings
plot(l,type = "n")
aps
head(s)
text(l,labels = names(s))
text(l,labels = names(s),xlim=c(0,1))
text(l,labels = names(s),xlim=c(0,1))
text(l,labels = names(s),xlim=c(0,1))
text(l,labels = names(s))
text(l,labels = names(s),xlim=c(-1,1))
plot(l,type = "n",xlim = c(0,1))
text(l,labels = names(s))
install.packages("psy")
library(psy)
scree.plot(fas$correlation)
library(psych)
scree(fas$correlation)
op<-read.csv("open_closed.csv")
View(op)
pca<-prcomp(op)
str(pca)
str(op)
summary(pca)
pairs.panels(pca$x)
pca2<-prcomp(op,scale=T)
summary(pca2)
plot(pca2,type="l")

#practice3
#1
bu<-read.csv("bulls.csv")
buu<-bu
bu<-bu[,-1]
bup<-prcomp(bu[,-1],scale=T)
bup
str(bup)
pairs.panels(bu[,-1])

#2
summary(bup)
plot(bup,type="l")

#3
plot(buu$SalePr,bup$x[,1])
cor(bup$x[,1],buu$SalePr)
plot(buu$SalePr,bup$x[,2])
cor(bup$x[,2],buu$SalePr)
plot(buu$SalePr,bup$x[,3])
cor(bup$x[,3],buu$SalePr)
plot(buu$SalePr,bup$x[,4])
cor(bup$x[,4],buu$SalePr)
plot(buu$SalePr,bup$x[,5])
cor(bup$x[,5],buu$SalePr)
plot(buu$SalePr,bup$x[,6])
cor(bup$x[,6],buu$SalePr)
plot(buu$SalePr,bup$x[,7])
cor(bup$x[,7],buu$SalePr)
bup$rotation[,c(1,2)]

#4
biplot(bup,xlim=c(-0.4,0.4))
pairs.panels(bu[,-1])
#5
b<-bup$x[,c(1,2)]
buu[,1]<-as.factor(buu[,1])
b<-as.data.frame(b)
b$Breed<-buu[,1]
head(b)
library(ggplot2)
g<-ggplot(b,aes(x=PC1,y=PC2,group=factor(Breed),color=factor(Breed),shape=factor(Breed)))+geom_point(size=3)
g
#6
qqnorm(b$PC1)
qqline(b$PC1)


#5.18 다변량 
ap<-read.table("Applicant.txt",header=T)
ap<-ap[,-1]
aps<-scale(ap)
install.packages("GPArotation")
library(GPArotation)
library(psych)

fa3<-fa(aps,4,rotate = "")
print(fa3,digits = 2,sort=T)

s<-read.csv("Stock_price.csv")
str(s)
s<-s[,-1]
head(aps)
aps2<-transform(aps,market=X11.AMB.+X8.SMS.)#인자 점수 
aps2


s<read.csv("")

#다변량 5.25
x=c(1,3,6,12,20) 
dist(x) #유클리드 거리

str(USArrests)
u<-USArrests
head(u)
us<-scale(u)
summary(us)
h<-hclust(dist(us),method = "complete") #최장 
summary(h)
plot(h)
h1<-hclust(dist(us),method = "single") #최단
plot(h1)
h2<-hclust(dist(us),method = "ward") #ward
plot(h2)

c<-cutree(h2,k=4) #4개의 그룹으로 나눠준다.
c1<-cutree(h2,h=8)
?cutree

cutree(h1,k=4) #이상치가 있는 경우 대부분이 한 집단에 속한다. 
table(cutree(h1,k=4)) # 집단에 속하는 개수 확인 

us<-data.frame(us)
plot(Murder~Assault,us,type="n")
text(us$Assault,us$Murder,rownames(us),col=c,cex=0.8)

usc<-c#cluster   #다시 
par(mfcol=c(2,2))
for (i in 1:4){
  boxplot(us[,i]~usc,main=names(us)[i])
}

gm<-aggregate(us[,1:4],by=list(usc),mean)
par(mfcol=c(2,2))
for (i in 1:4){
  barplot(as.matrix(gm[i,2:5]),las=2,main=paste("Group",i),ylim=c(-1.2,1.7))
}


km<-kmeans(us[-5],4)
km #within cluster 각 집단의 with ess(군집내 거리) 작을수록 좋다 
km$withinss

km$tot.withinss
ess<-NULL
for(k in 1:10){
  km<-kmeans(us[-5],k,nstart = 10)
  ess[k]<-km$tot.withinss
}
par(mfrow=c(1,1)) 
plot(ess,type="l")
points(ess)

install.packages("mclust")
library(mclust)  
mc<-Mclust(us[,-5])  
summary(mc) #가우시안은 정규분포와 같다
plot(mc)
1 #x축 클러스트의 갯
2

mc$classification

mc<-Mclust(us[,-5],G=4)
plot(mc)
1

#practice4

#1
s<-read.csv("Stock_price.csv")
str(s)
head(s)
tail(s)
s<-s[,-1]
ss<-scale(s)
factanal(ss,1)
factanal(ss,2)

sp<-prcomp(s,scale. = T)
summary(sp)
plot(sp,type="l")
head(ss)

#2
library(psy)
library(GPArotation)
fa1<-factanal(ss,2,rotation = "none")
print(fa1,digits = 2,sort=T) 

fa2<-factanal(ss,2) #varimax
print(fa2,digits = 2,sort=T) 

fa3<-factanal(ss,2,rotation = "quartimax")
print(fa3,digits = 2,sort=T) 

fa4<-factanal(ss,2,rotation = "oblimin")
print(fa4,digits = 2,sort=T) 

fa5<-factanal(ss,2,rotation = "promax")
print(fa5,digits = 2,sort=T) 

#4,#5
fa2<-factanal(ss,2)
print(fa2,digits = 2,sort=T) 
0.76^2
0.67^2+0.11^2
0.82^2+0.23^2
0.11^2+0.99^2
0.11^2+0.68^2

#6
ld<-fa2$loadings
plot(ld,type="n",xlim = c(0,1),xlab = "Financial factor",ylab="Oil factor")
name<-colnames(ss)
text(ld,labels = name)

#다변량 6.1
card<-read.csv("card.csv")
consumer<-read.csv("consumer.csv")
card<-reshape(card, varying=list(4:15), direction='long')
names(card)<-c("location", "category", "year", "month", "amt","id")
card$amt<-as.numeric(gsub(",","",card$amt))
card<-na.omit(card)


consumer<-reshape(consumer, varying = list(3:14), direction="long")
names(consumer)<-c("location",  "year", "month", "idx","id")
consumer<-na.omit(consumer)

library(stringr)
card$yearmonth<-paste(card$year, str_pad(card$month,2,pad="0"),"01",sep="-")
card$yearmonth<-strptime(card$yearmonth,"%Y-%m-%d")

consumer$yearmonth<-paste(consumer$year, str_pad(consumer$month,2,pad="0"),"01",sep="-")
consumer$yearmonth<-strptime(consumer$yearmonth,"%Y-%m-%d")

library(plyr)
card.month<-ddply(card,~category+yearmonth,summarize, amt=sum(amt,na.rm =TRUE))

# wide format으로 변환 
card.month$yearmonth2<-as.character(card.month$yearmonth)
card.month2<-reshape(card.month, direction="wide", timevar="category", idvar = "yearmonth2", v.names="amt")
names(card.month2)<-gsub("amt.","",names(card.month2))


library(ggplot2)
ggplot(card.month, aes(x=yearmonth, y=amt, color=category, group=category))+geom_line()
card.month.ratio=card.month2[-1,]
card.month.ratio[,-c(1,2)]=card.month.ratio[,-c(1,2)]/card.month2[-86, -c(1,2)]


matplot(card.month.ratio[,-c(1,2)],type='l')



library(psych)
scree(card.month.ratio[,-c(1,2)])
library(GPArotation)
fa1<-fa(card.month.ratio[,-c(1,2)],7,rotate="quartimax")
out=fa.sort(fa1)
plot(fa1)
fa.diagram(fa1,cut=0.2)
write.csv(out$loadings,"out.csv")

# factor score와 소비자동향지수 결합 
tmp<-consumer$yearmonth
consumer$yearmonth<-as.character(consumer$yearmonth)
consumer.t<-ddply(consumer, ~ yearmonth, summarize, idx=sum(idx,na.rm=TRUE))
consumer$yearmonth<-tmp
rm(tmp)

sc1<-data.frame(fa1$scores)
sc1$yearmonth<-card.month.ratio$yearmonth2
sc1<-merge(sc1,consumer.t, by="yearmonth")

cor(sc1[,2:8], sc1[,9])
lag=4
cor(sc1[(1+lag):86,2:8], sc1[1:(86-lag),9])

for (i in 1:7){
  ccf(sc1[,i+1],sc1[,9],main=paste("Factor Score",i))  
}

pca1<-prcomp(card.month.ratio[,-c(1,2)], scale=T)
summary(pca1)
biplot(pca1)

pca2<-prcomp(card.month2[,-c(1,2)], scale=T)
summary(pca2)
biplot(pca2)
par(mar=par()$mar+c(3,0,0,0))

for (i in 1:10){
  barplot(pca1$rotation[,i],las=2,main=paste("PC",i), ylim=c(-0.4,0.4))
}

card.loc<-ddply(card,~category+location,summarize, amt=sum(amt,na.rm=TRUE))

ggplot(card.loc, aes(x=location, y=amt, color=category, group=category))+geom_line()

ggplot(card.loc, aes(x=category, y=amt, color=location, group=location))+geom_line()


card.loc2<-reshape(card.loc, direction="wide", timevar="category", idvar="location")
names(card.loc2)<-gsub("amt.","",names(card.loc2))

rownames(card.loc2)<-card.loc2$location
card.loc2<-card.loc2[,-1]


card.loc.ratio<-t(scale(t(card.loc2)))
hc<-hclust(dist(card.loc.ratio),method="average")
plot(hc)
install.packages("mclust")
library(mclust)
mc<-Mclust(card.loc.ratio,5)

p
plot(mc)


###다변량 최종 프로젝트
library(data.table)
library(dplyr)
library(psych)

####데이터 부르기####
#역별 주소 부르기 add
add1=fread("서울교통공사 지하철 역별 주소 정보3.csv",header=T,encoding="UTF-8")
head(add1)

add2=fread("서울교통공사 지하철역 주소 및 전화번호 정보3.csv",header=T,encoding="UTF-8")
head(add2)

add1=subset(add1,duplicated(add1$역명)==F)
add2=subset(add2,duplicated(add2$역명)==F)
add1=subset(add1,add1$역명%in%setdiff(add1$역명,add2$역명))

add=rbind(add2,add1) #1~4호선과 5~8호선으로 나뉜 데이터 합침
length(unique(add$역명))
colnames(add)=c("Line","Station","Name")
rm(add1,add2)

#추정매출 부르기 sales
sales=fread("서울시 골목상권 프로파일 정보(상권-추정매출)3.csv",header=T,encoding="UTF-8")
sales[is.na(sales)]=0
sales=as.data.frame(sales)
colnames(sales)[1]="Name"
sales=sales[order(sales$Name),]
same=intersect(unique(sales$Name),unique(add$Name)) #역이 없는 상권은 제외
sales=sales[sales$Name%in%same,]

row.names(sales)<-1:76
pairs.panels(sales[,44:49]) ## 도로에 따른 연령별 매출 건수
pairs.panels(sales[,21:26])## 도로에 따른 연령별 매출 금액
pairs.panels(sales[,6:12]) ##도로에 따른 요일별 매출 금액 
pairs.panels(sales[,29:35]) ##도로에 따른 요일별 매출 건수 
pairs.panels(sales[,13:18]) ##도로에 따른 시간대별 매출 금액 
pairs.panels(sales[,36:41]) ## 도로에 따른 시간대별 매출 건수 

boxplot(sales[,c(4,5)]) #주말 주중 매출 금액 비교
boxplot(sales[,c(27,28)]) #주말 주중 매출 건수 비교
boxplot(sales[,c(42,43)]) #성별 매출 건수 비교
boxplot(sales[,c(19,20)]) #성별 매출 금액 비교  

###주성분 분석 
library(MVA)
pca1<-prcomp(sales[,2:49],scale. = T)
summary(pca1)
plot(pca1,type="l")
biplot(pca1,xlim=c(-0.6,0.6),cex=0.7)

####요인분석 EFA####
#sales
sales.efa=sales
rownames(sales.efa)=sales.efa$Name
sales.efa=sales.efa[,-1]
sales.efa=as.data.frame((scale(sales.efa))) 
fa.sales=factanal(sales.efa,10)
print(fa.sales) 
print(fa.sales,digits = 2,sort=T) 

load=fa.sales$loadings
plot(load,type = "n")
text(load,labels=names(sales.efa),cex=0.8)


#유동인구 부르기 pop
pop=fread("서울시 골목상권 프로파일 정보(상권-추정유동인구)3.csv",header=T,encoding="UTF-8")
pop[is.na(pop)]=0
pop=as.data.frame(pop)
pop=pop[order(pop$Name),]
pop=pop[pop$Name%in%same,] #역이 없는 상권은 제외
rm(same)


#상권 집객시설 부르기 home 집객시설 정의 : 손님을 모을수 있는것들 ex)영화관 백화점 등  

home<-fread("서울시 골목상권 프로파일 정보(상권-집객시설)3.csv",header=T,encoding="UTF-8")
home[is.na(home)]=0
home=as.data.frame(home)
home=home[order(home$Name),]
home1=home[home$Name%in%same,] #역이 없는 상권 집객시설은제외
row.names(home1)<-1:76
str(home1)
##데이터 탐색

home1$`개수 : 극장_수`<-NULL
home1$`개수 : 백화점_수`<-NULL
##요인분석 
home1.efa=home1
rownames(home1.efa)=home1.efa$Name
home1.efa=home1.efa[,-1]
home1.efa=as.data.frame((scale(home1.efa))) 
fa.home1=factanal(home1.efa,4) 
print(fa.home1) 
print(fa.home1,digits = 2,sort=T) 

load=fa.home1$loadings
plot(load,type = "n")
text(load,labels=names(home1.efa),cex=0.8)

library(psy)
scree.plot(fa.home1$correlation)  




#소득 소비 부르기 buy

buy<-fread("서울시 골목상권 프로파일링 정보(상권-소득소비)3.csv",header=T,encoding="UTF-8")
buy[is.na(buy)]=0
buy=as.data.frame(buy)
buy=buy[order(buy$Name),]
buy=buy[buy$Name%in%same,] #역이 없는 상권 집객시설은제외
row.names(buy)<-1:76
str(buy)
##데이터 탐색

library(GPArotation)
##요인분석 
buy.efa=buy
rownames(buy.efa)=buy.efa$Name
buy.efa=buy.efa[,-1]
buy.efa2<-buy.efa[,-1]
buy.efa2=as.data.frame((scale(buy.efa2))) 

fa.buy=factanal(buy.efa2,2,rotation = "quartimax")
print(fa.buy) 
print(fa.buy,digits = 2,sort=T) 

load=fa.buy$loadings
plot(load,type = "n")
text(load,labels=names(buy.efa),cex=0.8)

library(psy)
scree.plot(fa.buy$correlation)  



library(data.table)
library(dplyr)
library(psych)
library(psy)
library(GPArotation)

#### 데이터 부르기 ####
#역별 주소 부르기 add
add1=fread("서울교통공사 지하철 역별 주소 정보3.csv",header=T,encoding="UTF-8")
head(add1)

add2=fread("서울교통공사 지하철역 주소 및 전화번호 정보3.csv",header=T,encoding="UTF-8")
head(add2)

add1=subset(add1,duplicated(add1$역명)==F)
add2=subset(add2,duplicated(add2$역명)==F)
add1=subset(add1,add1$역명%in%setdiff(add1$역명,add2$역명))

add=rbind(add2,add1) #1~4호선과 5~8호선으로 나뉜 데이터 합침
length(unique(add$역명))
colnames(add)=c("Line","Station","Name")
rm(add1,add2)

#추정매출 부르기 sales
sales=fread("서울시 골목상권 프로파일 정보(상권-추정매출)3.csv",header=T,encoding="UTF-8")
sales[is.na(sales)]=0
sales=as.data.frame(sales)
colnames(sales)[1]="Name"
sales=sales[order(sales$Name),]
same=intersect(unique(sales$Name),unique(add$Name))
sales=sales[sales$Name%in%same,] #역이 없는 상권은 제외
rm(same)

#유동인구 부르기 pop
pop=fread("서울시 골목상권 프로파일 정보(상권-추정유동인구)3.csv",header=T,encoding="UTF-8")
pop[is.na(pop)]=0
pop=as.data.frame(pop)
pop=pop[order(pop$Name),]
same=intersect(unique(pop$Name),unique(add$Name))
pop=pop[pop$Name%in%same,] #역이 없는 상권은 제외
rm(same)

#집객시설 home
home<-fread("서울시 골목상권 프로파일 정보(상권-집객시설)3.csv",header=T,encoding="UTF-8")
home[is.na(home)]=0
home=as.data.frame(home)
home=home[order(home$Name),]
same=intersect(unique(pop$Name),unique(add$Name))
home=home[home$Name%in%same,] #역이 없는 상권은제외
row.names(home)<-1:76
rm(same)

#소득소비 cons
cons=fread("서울시 골목상권 프로파일링 정보(상권-소득소비)3.csv",header=T,encoding="UTF-8")
cons[is.na(cons)]=0
cons=as.data.frame(cons)
cons=cons[order(cons$Name),]
same=intersect(unique(pop$Name),unique(add$Name))
cons=cons[cons$Name%in%same,] #역이 없는 상권은 제외
rm(same)
head(cons)

#승하차 정보 부르기 io
io=fread("서울시 지하철 호선별 역별 시간대별 승하차 인원 정보.csv",header=T,encoding="UTF-8")
names=colnames(io) #너무 긴 필드 명 수정
names=gsub('시','h',names) 
names=gsub('승차인원','In',names)
names=gsub('하차인원','Out',names)
names[1:3]=c("Date","Line","Station")
colnames(io)=names;rm(names)
io=io[,-52]

#### 데이터 탐색 ####
#데이터 탐색 pop
pairs.panels(pop[2:7]) #연령별 상관관계
pairs.panels(pop[14:20]) #요일별 상관관계

#데이터 탐색 sales
pairs.panels(sales[,44:49]) ## 도로에 따른 연령별 매출 건수
pairs.panels(sales[,21:26])## 도로에 따른 연령별 매출 금액
pairs.panels(sales[,6:12]) ##도로에 따른 요일별 매출 금액
pairs.panels(sales[,29:35]) ##도로에 따른 요일별 매출 건수
pairs.panels(sales[,13:18]) ##도로에 따른 시간대별 매출 금액
pairs.panels(sales[,36:41]) ## 도로에 따른 시간대별 매출 건수
boxplot(sales[,c(4,5)]) #주말 주중 매출 금액 비교
boxplot(sales[,c(27,28)]) #주말 주중 매출 건수 비교
boxplot(sales[,c(42,43)]) #성별 매출 건수 비교
boxplot(sales[,c(19,20)]) #성별 매출 금액 비교 

#데이터 탐색 home
home$`개수 : 극장_수`<-NULL
home$`개수 : 백화점_수`<-NULL

#### EFA ####
#home 
home1.efa=home
rownames(home1.efa)=home1.efa$Name
home1.efa=home1.efa[,-1]
home1.efa=as.data.frame((scale(home1.efa))) 
fa.home1=factanal(home1.efa,4) 
print(fa.home1) 
print(fa.home1,digits = 2,sort=T) #p값이 작으므로 귀무가설을 기각하지 못한다. 따라서 factor가 설명력이 부족하다.

load=fa.home1$loadings
plot(load,type = "n")
text(load,labels=names(home1.efa),cex=0.8)

scree.plot(fa.home1$correlation)

rm(home1.efa,fa.home1,load)

#cons
cons.efa=cons
rownames(cons.efa)=cons.efa$Name
cons.efa=cons.efa[,-1]
cons.efa2<-cons.efa[,-1]
cons.efa2=as.data.frame((scale(cons.efa2))) 

fa.cons=factanal(cons.efa2,2,rotation = "quartimax")
print(fa.cons) 
print(fa.cons,digits = 2,sort=T) #p값이 작으므로 귀무가설을 기각하지 못한다. 따라서 factor가 설명력이 부족하다.

load=fa.cons$loadings
plot(load,type = "n")
text(load,labels=names(cons.efa),cex=0.8)

scree.plot(fa.cons$correlation)  

rm(fa.cons,cons.efa,cons.efa2,load)

#### PCA ####
#sales
sales1=sales
rownames(sales1)=sales1$Name
sales1=sales1[,-1]
pca.sales=prcomp(sales1,scale=T)

summary(pca.sales)
plot(pca.sales,type='l') #2개 선택 

biplot(pca.sales, xlim=c(-0.6,0.6),cex=0.7) #연령대 20 매출금액, 시간대 06-11 매출금액

par(mfrow=c(4,5))
for(i in 1:20){
  plot(pca.sales$x[,i])
}

par(mfrow=c(1,1))
result<-pca.sales$x[,1:2]
result<-as.data.frame(result)
qqnorm(result$PC1)
qqline(result$PC1)

sales1=sales[c(1,14,22)] #주성분만 사용한 데이터

rm(pca.sales)

#pop
pop1=pop
rownames(pop1)=pop1$Name
pop1=pop1[,-1]
pca.pop=prcomp(pop1,scale=T)

summary(pca.pop)
plot(pca.pop,type='l') #2개 선택

biplot(pca.pop, xlim=c(-0.6,0.6),cex=0.7) #연령대 20 유동인구수, 일요일 유동인구 수

par(mfrow=c(4,5))
for(i in 1:20){
  plot(pca.pop$x[,i])
};rm(i)
par(mfrow=c(1,1))
result<-pca.pop$x[,1:2]
result<-as.data.frame(result)
qqnorm(result$PC1)
qqline(result$PC1)

pop1=pop[c(1,3,20)] #주성분만 사용한 데이터

rm(pca.pop,result)

#cons
cons1=cons
rownames(cons1)=cons1$Name
cons1=cons1[,-1]
pca.cons=prcomp(cons1,scale=T)

summary(pca.cons)
plot(pca.cons,type='l') #1개 선택

biplot(pca.cons, xlim=c(-0.6,0.6),cex=0.7)

result<-pca.cons$x[,1:2]
result<-as.data.frame(result)
qqnorm(result$PC1)
qqline(result$PC1)

cons1=cons[c(1,2)]
rm(result,pca.cons)

#home
home1=home
rownames(home1)=home1$Name
home1=home1[,-1]
pca.home=prcomp(home1,scale=T)

summary(pca.home)
plot(pca.home,type='l') #4개 선택

biplot(pca.home,cex=0.7)

result<-pca.home$x[,1:4]
result<-as.data.frame(result)
qqnorm(result$PC1)
qqline(result$PC1)

home1=home[c(1,2,5,8,10)]
rm(result,pca.home)


#### 군집분석 ####
ro=left_join(cons1,home1)%>%
  left_join(pop1)%>%left_join(sales1)

#kmeans
km=kmeans(ro[,-1],4,nstart = 1)
ess=c()
for(k in 1:10){
  km=kmeans(ro[,-1],k,nstart = 10)
  ess[k]=km$tot.withinss
}
plot(ess,type = 'l')
points(ess)

km=kmeans(ro[-1],centers = 3)
plot(ro$`평균 : 지출_총금액`,ro$`평균 : 연령대_20_유동인구_수`,pch=km$cluster,col=km$cluster)
text(ro$`평균 : 지출_총금액`,ro$`평균 : 연령대_20_유동인구_수`,labels=ro$Name,cex=0.8,adj=0,pos=4,col=km$cluster)
points(km$centers[,c(1,3)],col=1:3,pch=4,cex=2)



#### 승하차 데이터 전처리 ####
str(io)
io=as.data.frame(io)
for(i in 4:51){
  io[,i]=as.integer(io[,i]) #문자열을 숫자로 바꿔줌
};rm(i)

same=intersect(unique(sales$Name),unique(add$Name))
add1=add[add$Name%in%same] #골목상권을 기반으로 국민대역이 어떤 역이 될 지 예측하는 것이기 떄문에 골목상권이 없는 역은 제외한다.
length(unique(add1$Name)) 
rm(same)



